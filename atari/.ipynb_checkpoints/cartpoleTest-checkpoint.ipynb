{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE ARRAY AS INPUT\n",
    "\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "from environment import City\n",
    "\n",
    "import pickle\n",
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense, Conv2D, Conv3D, Flatten, MaxPooling2D, MaxPooling3D\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import adam\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False\n",
    "exploration = True\n",
    "load_model = False\n",
    "\n",
    "\n",
    "def transform_reward(reward):\n",
    "    return np.sign(reward)\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env = gym.make('CartPole-v0')\n",
    "# Reset it, returns the starting frame\n",
    "frame = env.reset()\n",
    "print('[INFO] Shape input:', frame.shape)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    \"\"\" Implementation of deep q learning algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, action_space, state_space, debug_mode = False, load_model = True):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1\n",
    "        # discount value \n",
    "        # 0 for present and 1 for future\n",
    "        self.gamma = .2\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        # epsilon denotes the fraction of time we will dedicate to exploring\n",
    "        #self.epsilon_min = .01\n",
    "        self.epsilon_min = .01\n",
    "        self.epsilon_decay = .995\n",
    "        self.learning_rate = 0.01\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        if load_model:\n",
    "            print(\"[INFO] Loading model from disk\")\n",
    "            # load json and create model\n",
    "            json_file = open('models/cartpole.json', 'r')\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "            self.model = model_from_json(loaded_model_json)\n",
    "            # load weights into new model\n",
    "            self.model.load_weights(\"models/cartpole.h5\")\n",
    "            self.model.compile(loss='mse', optimizer=adam(lr=self.learning_rate))\n",
    "            print(\"[INFO] Model loaded\")\n",
    "            return\n",
    "        #self.model = self.atari_model()\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    \n",
    "    def build_model1(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=(self.state_space,), activation='relu'))\n",
    "        #model.add(Dense(64, input_dim=2, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_model(self):\n",
    "        input_shape= (self.state_space,)\n",
    "        action_space= (self.action_space)\n",
    "        X_input = Input(input_shape)\n",
    "\n",
    "        # 'Dense' is the basic form of a neural network layer\n",
    "        # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "        X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "\n",
    "        # Hidden layer with 256 nodes\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        # Hidden layer with 64 nodes\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        # Output Layer with # of actions: 2 nodes (left, right)\n",
    "        X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if debug_mode:    \n",
    "            print('State: \\n', state[0].reshape(y_size, x_size))\n",
    "            print('action: ', action)\n",
    "            print('Next state: \\n', next_state[0].reshape(y_size, x_size))\n",
    "            print('reward: ', reward)\n",
    "            print('------------------------------------------------')\n",
    "\n",
    "    def act(self, state):\n",
    "        # if the random float is smaller than epsilon reduced, it takes a random action (explore)\n",
    "        if np.random.rand() <= self.epsilon and exploration:\n",
    "            #print('Exploration step')\n",
    "            return random.randrange(self.action_space)\n",
    "        # else exploit\n",
    "        #state = state.reshape((1, 2, 105, 80, 1))\n",
    "        state = state.reshape((1,self.state_space))\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "        \n",
    "        '''\n",
    "        print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "        print('States -------------------------------------------------------------------------')\n",
    "        plt.imshow(states[0][0].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        plt.imshow(states[0][1].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        \n",
    "        print('Next states -------------------------------------------------------------------------')\n",
    "        plt.imshow(next_states[0][0].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        plt.imshow(next_states[0][1].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # First, predict the Q values of the next states. Note how we are passing ones as the mask.\n",
    "        next_Q_values = self.model.predict(next_states)\n",
    "        # The Q values of the terminal states is 0 by definition, so override them\n",
    "        next_Q_values[dones] = 0\n",
    "        # The Q values of each start state is the reward + gamma * the max next state Q value\n",
    "        targets = rewards + self.gamma * np.max(next_Q_values, axis=1)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        \n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        # every new iteration reduce epsilon to push the exploration\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return self.model\n",
    "    \n",
    "    \n",
    "\n",
    "def train_dqn(episode):\n",
    "    print(episode)\n",
    "    loss = []\n",
    "    agent = DQN(2, 4, debug_mode, load_model)\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        #set_states = np.array([state, state])\n",
    "        score = 0\n",
    "        max_steps = 10000\n",
    "        \n",
    "        #set_states_past = np.array([state, state])\n",
    "        #set_states_fut = np.array([state])\n",
    "        \n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            \n",
    "            #print(state.shape)\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            #action = agent.act(set_states)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            #set_states_fut = np.array([set_states_fut[-1], next_state])\n",
    "            \n",
    "            score += reward\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            #agent.remember(set_states_past, action, reward, set_states_fut, done)\n",
    "            state = next_state\n",
    "\n",
    "            #set_states_past = np.array([set_states_past[-1], next_state])\n",
    "\n",
    "            \n",
    "            model = agent.replay()\n",
    "            env.render()\n",
    "            if done:\n",
    "                break\n",
    "        loss.append(score)\n",
    "        print(\"episode: {}/{}, moves:{}, score: {}\".format(e, episode, i, str(score)[:4]))\n",
    "        if (e+1) % 100 == 0:\n",
    "            '''\n",
    "            print('[INFO] Saving checkpoint iter:', e)\n",
    "            # serialize model to JSON\n",
    "            model_json = model.to_json()\n",
    "            with open(\"models/cartpole.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(\"models/cartpole.h5\")\n",
    "            print(\"[INFO] Saved model to disk\")\n",
    "            # with open(r\"models/model_auto6.pickle\", \"wb\") as f:\n",
    "            #    pickle.dump(agent, f)\n",
    "            '''\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot([i for i in range(e)], loss[-e:])\n",
    "            plt.xlabel('episodes')\n",
    "            plt.ylabel('reward')\n",
    "            #plt.savefig('training_graph_check{}'.format(e))\n",
    "            plt.show()\n",
    "    return loss\n",
    "\n",
    "ep = 100000\n",
    "loss = train_dqn(ep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "\n",
    "    # Hidden layer with 256 nodes\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    \n",
    "    # Hidden layer with 64 nodes\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        #self.env = gym.make('MountainCar-v0')\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        # by default, CartPole-v1 has max episode steps = 500\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        \n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create main model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.train_start:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # Standard - DQN\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                # selection and evaluation of action is on the target Q Network\n",
    "                # Q_max = max_a' Q_target(s', a')\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "            \n",
    "    def run(self):\n",
    "        self.load(\"cartpole-dqn.h5\")\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                if not done or i == self.env._max_episode_steps-1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                score += reward\n",
    "                if done:                   \n",
    "                    print(\"episode: {}/{}, steps: {}, score: {}, e: {:.2}\".format(e, self.EPISODES, i, score, self.epsilon))\n",
    "                    if e == 1000:\n",
    "                        print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                        self.save(\"cartpole-dqn.h5\")\n",
    "                        return\n",
    "                self.replay()\n",
    "\n",
    "    def test(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
    "                    break\n",
    "\n",
    "                    \n",
    "agent = DQNAgent()\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SumTree(object):\n",
    "    data_pointer = 0\n",
    "    \n",
    "    # Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    def __init__(self, capacity):\n",
    "        # Number of leaf nodes (final nodes) that contains experiences\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema below\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    # Here we define function that will add our priority score in the sumtree leaf and add the experience in data:\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "\n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "\n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "\n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, we go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    # Update the leaf priority score and propagate the change through tree\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # then propagate the change through tree\n",
    "        # this method is faster than the recursive loop in the reference code\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "        \n",
    "    # Here build a function to get a leaf from our tree. So we'll build a function to get the leaf_index, priority value of that leaf and experience associated with that leaf index:\n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "\n",
    "        # the while loop is faster than the method in the reference code\n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "\n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else: # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node\n",
    "\n",
    "# Now we finished constructing our SumTree object, next we'll build a memory object.\n",
    "class Memory(object):  # stored as ( state, action, reward, next_state ) in SumTree\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    # Next, we define a function to store a new experience in our tree.\n",
    "    # Each new experience will have a score of max_prority (it will be then improved when we use this exp to train our DDQN).\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "\n",
    "        # If the max priority = 0 we can't put priority = 0 since this experience will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "\n",
    "        self.tree.add(max_priority, experience)   # set the max priority for new priority\n",
    "        \n",
    "    # Now we create sample function, which will be used to pick batch from our tree memory, which will be used to train our model.\n",
    "    # - First, we sample a minibatch of n size, the range [0, priority_total] into priority ranges.\n",
    "    # - Then a value is uniformly sampled from each range.\n",
    "    # - Then we search in the sumtree, for the experience where priority score correspond to sample values are retrieved from.\n",
    "    def sample(self, n):\n",
    "        # Create a minibatch array that will contains the minibatch\n",
    "        minibatch = []\n",
    "\n",
    "        b_idx = np.empty((n,), dtype=np.int32)\n",
    "\n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "\n",
    "        for i in range(n):\n",
    "            # A value is uniformly sample from each range\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "\n",
    "            # Experience that correspond to each value is retrieved\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "\n",
    "            b_idx[i]= index\n",
    "\n",
    "            minibatch.append([data[0],data[1],data[2],data[3],data[4]])\n",
    "\n",
    "        return b_idx, minibatch\n",
    "    \n",
    "    # Update the priorities on the tree\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__ is 2.1.0\n",
      "tf.keras.__version__ is: 2.2.4-tf\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "\n",
    "print(\"tf.__version__ is\", tf.__version__)\n",
    "print(\"tf.keras.__version__ is:\", tf.keras.__version__)\n",
    "\n",
    "def _get_available_gpus():\n",
    "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
    "\n",
    "    # Returns\n",
    "        A list of available GPU devices.\n",
    "    \"\"\"\n",
    "    #global _LOCAL_DEVICES\n",
    "    if tfback._LOCAL_DEVICES is None:\n",
    "        devices = tf.config.list_logical_devices()\n",
    "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "\n",
    "tfback._get_available_gpus = _get_available_gpus\n",
    "\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lorenzo/anaconda3/envs/lorenzo_env/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From /home/lorenzo/anaconda3/envs/lorenzo_env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4, 80, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 19, 19)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 8, 8)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 6, 6)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 1,406,180\n",
      "Trainable params: 1,406,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4, 80, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 19, 19)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 8, 8)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 6, 6)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 1,406,180\n",
      "Trainable params: 1,406,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0/1000, score: 4.0, e: 0.99, average: 4.00 SAVING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2e192d0cbc51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;31m#env_name = 'Pong-v0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0;31m#agent.test('Models/Pong-v0DQN__CNN.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2e192d0cbc51>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                     \u001b[0;31m# every episode, plot the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                     \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPlotModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                     \u001b[0;31m# saving best models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2e192d0cbc51>\u001b[0m in \u001b[0;36mPlotModel\u001b[0;34m(self, score, episode)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSE_PER\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_PER'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdueling\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mgreedy\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPER\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_CNN.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2092\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1709\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/spines.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adjust_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    578\u001b[0m                       \u001b[0;31m# do not draw the hatches if the facecolor is fully\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                       \u001b[0;31m# transparent, but do if it is None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m                       self._facecolor if self._facecolor[3] else None)\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw_path\u001b[0;34m(self, gc, path, transform, rgbFace)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgbFace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOverflowError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 raise OverflowError(\"Exceeded cell block limit (set \"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tutorial by www.pylessons.com\n",
    "# Tutorial written for - Tensorflow 1.15, Keras 2.2.4\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def OurModel(input_shape, action_space, dueling):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    #X = Conv2D(64, 5, strides=(3, 3),padding=\"valid\", input_shape=input_shape, activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(32, 8, strides=(4, 4),padding=\"valid\", input_shape=input_shape, activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(64, 4, strides=(2, 2),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(64, 3, strides=(1, 1),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Flatten()(X)\n",
    "    \n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    X = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    if dueling:\n",
    "        state_value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "        state_value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(action_space,))(state_value)\n",
    "\n",
    "        action_advantage = Dense(action_space, kernel_initializer='he_uniform')(X)\n",
    "        action_advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(action_space,))(action_advantage)\n",
    "\n",
    "        X = Add()([state_value, action_advantage])\n",
    "    else:\n",
    "        # Output Layer with # of actions: 2 nodes (left, right)\n",
    "        X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    #model.compile(loss=\"mean_squared_error\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "    #model.compile(optimizer=Adam(lr=0.00025), loss='mean_squared_error')\n",
    "    model.compile(optimizer=Adam(lr=0.00005), loss='mean_squared_error')\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env_name):\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        #self.env.seed(0)  \n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        \n",
    "        # Instantiate memory\n",
    "        memory_size = 25000\n",
    "        self.MEMORY = Memory(memory_size)\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        \n",
    "        # EXPLORATION HYPERPARAMETERS for epsilon and epsilon greedy strategy\n",
    "        self.epsilon = 1.0  # exploration probability at start\n",
    "        self.epsilon_min = 0.02  # minimum exploration probability \n",
    "        self.epsilon_decay = 0.00002  # exponential decay rate for exploration prob\n",
    "        \n",
    "        self.batch_size = 32\n",
    "\n",
    "        # defining model parameters\n",
    "        self.ddqn = False # use doudle deep q network\n",
    "        self.dueling = False # use dealing netowrk\n",
    "        self.epsilon_greedy = False # use epsilon greedy strategy\n",
    "        self.USE_PER = False # use priority experienced replay\n",
    "        \n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name+\"_CNN.h5\")\n",
    "\n",
    "        self.ROWS = 80\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 4\n",
    "        self.update_model_steps = 1000\n",
    "        \n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "        \n",
    "        # create main model and target model\n",
    "        self.model = OurModel(input_shape=self.state_size, action_space = self.action_size, dueling = self.dueling)\n",
    "        self.target_model = OurModel(input_shape=self.state_size, action_space = self.action_size, dueling = self.dueling)  \n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self, game_steps):\n",
    "        if game_steps % self.update_model_steps == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            return\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        experience = state, action, reward, next_state, done\n",
    "        if self.USE_PER:\n",
    "            self.MEMORY.store(experience)\n",
    "        else:\n",
    "            self.memory.append((experience))\n",
    "\n",
    "    def act(self, state, decay_step):\n",
    "        # EPSILON GREEDY STRATEGY\n",
    "        if self.epsilon_greedy:\n",
    "        # Here we'll use an improved version of our epsilon greedy strategy for Q-learning\n",
    "            explore_probability = self.epsilon_min + (self.epsilon - self.epsilon_min) * np.exp(-self.epsilon_decay * decay_step)\n",
    "        # OLD EPSILON STRATEGY\n",
    "        else:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= (1-self.epsilon_decay)\n",
    "            explore_probability = self.epsilon\n",
    "    \n",
    "        if explore_probability > np.random.rand():\n",
    "            # Make a random action (exploration)\n",
    "            return random.randrange(self.action_size), explore_probability\n",
    "        else:\n",
    "            # Get action from Q-network (exploitation)\n",
    "            # Estimate the Qs values state\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            return np.argmax(self.model.predict(state)), explore_probability\n",
    "                \n",
    "    def replay(self):\n",
    "        if self.USE_PER:\n",
    "            # Sample minibatch from the PER memory\n",
    "            tree_idx, minibatch  = self.MEMORY.sample(self.batch_size)\n",
    "        else:\n",
    "            if len(self.memory) > self.batch_size:\n",
    "            # Randomly sample minibatch from the deque memory\n",
    "                minibatch = random.sample(self.memory, self.batch_size)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "        state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        action = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        reward = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        next_state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        done = np.zeros(self.batch_size, dtype=np.uint8)\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop       \n",
    "        for i in range(len(minibatch)):\n",
    "            state[i], action[i], reward[i], next_state[i], done[i] = minibatch[i]\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        # predict Q-values for starting state using the main network\n",
    "        target = self.model.predict(state)\n",
    "        target_old = np.array(target)\n",
    "        # predict best action in ending state using the main network\n",
    "        target_next = self.model.predict(next_state)\n",
    "        # predict Q-values for ending state using the target network\n",
    "        target_val = self.target_model.predict(next_state)\n",
    "\n",
    "\n",
    "        for i in range(len(minibatch)):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                if self.ddqn: # Double - DQN\n",
    "                    # current Q Network selects the action\n",
    "                    # a'_max = argmax_a' Q(s', a')\n",
    "                    a = np.argmax(target_next[i])\n",
    "                    # target Q Network evaluates the action\n",
    "                    # Q_max = Q_target(s', a'_max)\n",
    "                    target[i][action[i]] = reward[i] + self.gamma * target_val[i][a]\n",
    "                else: # Standard - DQN\n",
    "                    # DQN chooses the max Q value among next actions\n",
    "                    # selection and evaluation of action is on the target Q Network\n",
    "                    # Q_max = max_a' Q_target(s', a')\n",
    "                    # when using target model in simple DQN rules, we get better performance\n",
    "                    target[i][action[i]] = reward[i] + self.gamma * np.amax(target_val[i])\n",
    "            \n",
    "        if self.USE_PER:\n",
    "            indices = np.arange(self.batch_size, dtype=np.int32)\n",
    "            absolute_errors = np.abs(target_old[indices, action]-target[indices, action])\n",
    "\n",
    "            # Update priority\n",
    "            self.MEMORY.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        return\n",
    "        self.model.save(name)\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        pylab.plot(self.episodes, self.average, 'r')\n",
    "        pylab.plot(self.episodes, self.scores, 'b')\n",
    "        pylab.ylabel('Score', fontsize=18)\n",
    "        pylab.xlabel('Games', fontsize=18)\n",
    "        dqn = 'DQN_'\n",
    "        dueling = ''\n",
    "        greedy = ''\n",
    "        PER = ''\n",
    "        if self.ddqn: dqn = '_DDQN'\n",
    "        if self.dueling: dueling = '_Dueling'\n",
    "        if self.epsilon_greedy: greedy = '_Greedy'\n",
    "        if self.USE_PER: PER = '_PER'\n",
    "        try:\n",
    "            pylab.savefig(self.env_name+dqn+dueling+greedy+PER+\"_CNN.png\")\n",
    "        except OSError:\n",
    "            pass\n",
    "        # no need to worry about model, when doing a lot of experiments\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name+dqn+dueling+greedy+PER+\"_CNN.h5\")\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(\"cartpole\"+str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def GetImage(self, frame):\n",
    "        self.env.render()\n",
    "        \n",
    "        # croping frame to 80x80 size\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "            # OpenCV resize function \n",
    "            frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # converting to RGB (numpy way)\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "        # converting to RGB (OpenCV way)\n",
    "        #frame_rgb = cv2.cvtColor(frame_cropped, cv2.COLOR_RGB2GRAY)     \n",
    "\n",
    "        # dividing by 255 we expresses value to 0-1 representation\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        self.image_memory[0,:,:] = new_frame\n",
    "\n",
    "        # show image frame   \n",
    "        #self.imshow(self.image_memory,0)\n",
    "        #self.imshow(self.image_memory,1)\n",
    "        #self.imshow(self.image_memory,2)\n",
    "        #self.imshow(self.image_memory,3)\n",
    "        \n",
    "        return np.expand_dims(self.image_memory, axis=0)\n",
    "\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(frame)\n",
    "        return state\n",
    "\n",
    "    def step(self,action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = self.GetImage(next_state)\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def run(self):\n",
    "        decay_step = 0\n",
    "        max_average = -21.0\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            SAVING = ''\n",
    "            while not done:\n",
    "                decay_step += 1\n",
    "                action, explore_probability = self.act(state, decay_step)\n",
    "                next_state, reward, done, _ = self.step(action)\n",
    "                '''\n",
    "                if reward != 0.0:\n",
    "                    print('States -------------------------------------------------------------------------')\n",
    "                    plt.imshow(state[0][0])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][1])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][2])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][3])\n",
    "                    plt.show()\n",
    "                    print('Action:', action)\n",
    "                    print('Reward:', reward)\n",
    "                    '''\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    # every episode, plot the result\n",
    "                    average = self.PlotModel(score, e)\n",
    "\n",
    "                    # saving best models\n",
    "                    if average >= max_average:\n",
    "                        max_average = average\n",
    "                        self.save(self.Model_name)\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2f}, average: {:.2f} {}\".format(e, self.EPISODES, score, explore_probability, average, SAVING))\n",
    "                    \n",
    "                # update target model\n",
    "                self.update_target_model(decay_step)\n",
    "\n",
    "                # train model\n",
    "                self.replay()\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self, Model_name):\n",
    "        import matplotlib.pyplot as plt    \n",
    "        import time\n",
    "        self.load(Model_name)\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                time.sleep(.01)\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = self.GetImage(next_state)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "        self.env.close()\n",
    "\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    #env_name = 'BreakoutDeterministic-v4'\n",
    "    env_name = 'Pong-v0'\n",
    "    agent = DQNAgent(env_name)\n",
    "    #agent.run()\n",
    "    agent.test('Models/Pong-v0DQN__CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lorenzo_env] *",
   "language": "python",
   "name": "conda-env-lorenzo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
