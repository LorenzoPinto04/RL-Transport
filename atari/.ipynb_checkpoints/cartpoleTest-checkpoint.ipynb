{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE ARRAY AS INPUT\n",
    "\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "from environment import City\n",
    "\n",
    "import pickle\n",
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense, Conv2D, Conv3D, Flatten, MaxPooling2D, MaxPooling3D\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import adam\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False\n",
    "exploration = True\n",
    "load_model = False\n",
    "\n",
    "\n",
    "def transform_reward(reward):\n",
    "    return np.sign(reward)\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env = gym.make('CartPole-v0')\n",
    "# Reset it, returns the starting frame\n",
    "frame = env.reset()\n",
    "print('[INFO] Shape input:', frame.shape)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    \"\"\" Implementation of deep q learning algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, action_space, state_space, debug_mode = False, load_model = True):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1\n",
    "        # discount value \n",
    "        # 0 for present and 1 for future\n",
    "        self.gamma = .2\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        # epsilon denotes the fraction of time we will dedicate to exploring\n",
    "        #self.epsilon_min = .01\n",
    "        self.epsilon_min = .01\n",
    "        self.epsilon_decay = .995\n",
    "        self.learning_rate = 0.01\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        if load_model:\n",
    "            print(\"[INFO] Loading model from disk\")\n",
    "            # load json and create model\n",
    "            json_file = open('models/cartpole.json', 'r')\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "            self.model = model_from_json(loaded_model_json)\n",
    "            # load weights into new model\n",
    "            self.model.load_weights(\"models/cartpole.h5\")\n",
    "            self.model.compile(loss='mse', optimizer=adam(lr=self.learning_rate))\n",
    "            print(\"[INFO] Model loaded\")\n",
    "            return\n",
    "        #self.model = self.atari_model()\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    \n",
    "    def build_model1(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=(self.state_space,), activation='relu'))\n",
    "        #model.add(Dense(64, input_dim=2, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_model(self):\n",
    "        input_shape= (self.state_space,)\n",
    "        action_space= (self.action_space)\n",
    "        X_input = Input(input_shape)\n",
    "\n",
    "        # 'Dense' is the basic form of a neural network layer\n",
    "        # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "        X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "\n",
    "        # Hidden layer with 256 nodes\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        # Hidden layer with 64 nodes\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        # Output Layer with # of actions: 2 nodes (left, right)\n",
    "        X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if debug_mode:    \n",
    "            print('State: \\n', state[0].reshape(y_size, x_size))\n",
    "            print('action: ', action)\n",
    "            print('Next state: \\n', next_state[0].reshape(y_size, x_size))\n",
    "            print('reward: ', reward)\n",
    "            print('------------------------------------------------')\n",
    "\n",
    "    def act(self, state):\n",
    "        # if the random float is smaller than epsilon reduced, it takes a random action (explore)\n",
    "        if np.random.rand() <= self.epsilon and exploration:\n",
    "            #print('Exploration step')\n",
    "            return random.randrange(self.action_space)\n",
    "        # else exploit\n",
    "        #state = state.reshape((1, 2, 105, 80, 1))\n",
    "        state = state.reshape((1,self.state_space))\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "        \n",
    "        '''\n",
    "        print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "        print('States -------------------------------------------------------------------------')\n",
    "        plt.imshow(states[0][0].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        plt.imshow(states[0][1].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        \n",
    "        print('Next states -------------------------------------------------------------------------')\n",
    "        plt.imshow(next_states[0][0].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        plt.imshow(next_states[0][1].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # First, predict the Q values of the next states. Note how we are passing ones as the mask.\n",
    "        next_Q_values = self.model.predict(next_states)\n",
    "        # The Q values of the terminal states is 0 by definition, so override them\n",
    "        next_Q_values[dones] = 0\n",
    "        # The Q values of each start state is the reward + gamma * the max next state Q value\n",
    "        targets = rewards + self.gamma * np.max(next_Q_values, axis=1)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        \n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        # every new iteration reduce epsilon to push the exploration\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return self.model\n",
    "    \n",
    "    \n",
    "\n",
    "def train_dqn(episode):\n",
    "    print(episode)\n",
    "    loss = []\n",
    "    agent = DQN(2, 4, debug_mode, load_model)\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        #set_states = np.array([state, state])\n",
    "        score = 0\n",
    "        max_steps = 10000\n",
    "        \n",
    "        #set_states_past = np.array([state, state])\n",
    "        #set_states_fut = np.array([state])\n",
    "        \n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            \n",
    "            #print(state.shape)\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            #action = agent.act(set_states)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            #set_states_fut = np.array([set_states_fut[-1], next_state])\n",
    "            \n",
    "            score += reward\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            #agent.remember(set_states_past, action, reward, set_states_fut, done)\n",
    "            state = next_state\n",
    "\n",
    "            #set_states_past = np.array([set_states_past[-1], next_state])\n",
    "\n",
    "            \n",
    "            model = agent.replay()\n",
    "            env.render()\n",
    "            if done:\n",
    "                break\n",
    "        loss.append(score)\n",
    "        print(\"episode: {}/{}, moves:{}, score: {}\".format(e, episode, i, str(score)[:4]))\n",
    "        if (e+1) % 100 == 0:\n",
    "            '''\n",
    "            print('[INFO] Saving checkpoint iter:', e)\n",
    "            # serialize model to JSON\n",
    "            model_json = model.to_json()\n",
    "            with open(\"models/cartpole.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(\"models/cartpole.h5\")\n",
    "            print(\"[INFO] Saved model to disk\")\n",
    "            # with open(r\"models/model_auto6.pickle\", \"wb\") as f:\n",
    "            #    pickle.dump(agent, f)\n",
    "            '''\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot([i for i in range(e)], loss[-e:])\n",
    "            plt.xlabel('episodes')\n",
    "            plt.ylabel('reward')\n",
    "            #plt.savefig('training_graph_check{}'.format(e))\n",
    "            plt.show()\n",
    "    return loss\n",
    "\n",
    "ep = 100000\n",
    "loss = train_dqn(ep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "\n",
    "    # Hidden layer with 256 nodes\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    \n",
    "    # Hidden layer with 64 nodes\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        #self.env = gym.make('MountainCar-v0')\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        # by default, CartPole-v1 has max episode steps = 500\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        \n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create main model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.train_start:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # Standard - DQN\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                # selection and evaluation of action is on the target Q Network\n",
    "                # Q_max = max_a' Q_target(s', a')\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "            \n",
    "    def run(self):\n",
    "        self.load(\"cartpole-dqn.h5\")\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                if not done or i == self.env._max_episode_steps-1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                score += reward\n",
    "                if done:                   \n",
    "                    print(\"episode: {}/{}, steps: {}, score: {}, e: {:.2}\".format(e, self.EPISODES, i, score, self.epsilon))\n",
    "                    if e == 1000:\n",
    "                        print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                        self.save(\"cartpole-dqn.h5\")\n",
    "                        return\n",
    "                self.replay()\n",
    "\n",
    "    def test(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
    "                    break\n",
    "\n",
    "                    \n",
    "agent = DQNAgent()\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SumTree(object):\n",
    "    data_pointer = 0\n",
    "    \n",
    "    # Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    def __init__(self, capacity):\n",
    "        # Number of leaf nodes (final nodes) that contains experiences\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema below\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    # Here we define function that will add our priority score in the sumtree leaf and add the experience in data:\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "\n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "\n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "\n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, we go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    # Update the leaf priority score and propagate the change through tree\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # then propagate the change through tree\n",
    "        # this method is faster than the recursive loop in the reference code\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "        \n",
    "    # Here build a function to get a leaf from our tree. So we'll build a function to get the leaf_index, priority value of that leaf and experience associated with that leaf index:\n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "\n",
    "        # the while loop is faster than the method in the reference code\n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "\n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else: # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node\n",
    "\n",
    "# Now we finished constructing our SumTree object, next we'll build a memory object.\n",
    "class Memory(object):  # stored as ( state, action, reward, next_state ) in SumTree\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    # Next, we define a function to store a new experience in our tree.\n",
    "    # Each new experience will have a score of max_prority (it will be then improved when we use this exp to train our DDQN).\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "\n",
    "        # If the max priority = 0 we can't put priority = 0 since this experience will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "\n",
    "        self.tree.add(max_priority, experience)   # set the max priority for new priority\n",
    "        \n",
    "    # Now we create sample function, which will be used to pick batch from our tree memory, which will be used to train our model.\n",
    "    # - First, we sample a minibatch of n size, the range [0, priority_total] into priority ranges.\n",
    "    # - Then a value is uniformly sampled from each range.\n",
    "    # - Then we search in the sumtree, for the experience where priority score correspond to sample values are retrieved from.\n",
    "    def sample(self, n):\n",
    "        # Create a minibatch array that will contains the minibatch\n",
    "        minibatch = []\n",
    "\n",
    "        b_idx = np.empty((n,), dtype=np.int32)\n",
    "\n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "\n",
    "        for i in range(n):\n",
    "            # A value is uniformly sample from each range\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "\n",
    "            # Experience that correspond to each value is retrieved\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "\n",
    "            b_idx[i]= index\n",
    "\n",
    "            minibatch.append([data[0],data[1],data[2],data[3],data[4]])\n",
    "\n",
    "        return b_idx, minibatch\n",
    "    \n",
    "    # Update the priorities on the tree\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__ is 2.1.0\n",
      "tf.keras.__version__ is: 2.2.4-tf\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "\n",
    "print(\"tf.__version__ is\", tf.__version__)\n",
    "print(\"tf.keras.__version__ is:\", tf.keras.__version__)\n",
    "\n",
    "def _get_available_gpus():\n",
    "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
    "\n",
    "    # Returns\n",
    "        A list of available GPU devices.\n",
    "    \"\"\"\n",
    "    #global _LOCAL_DEVICES\n",
    "    if tfback._LOCAL_DEVICES is None:\n",
    "        devices = tf.config.list_logical_devices()\n",
    "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "\n",
    "tfback._get_available_gpus = _get_available_gpus\n",
    "\n",
    "'''\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2600)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 4, 80, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 19, 19)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 8, 8)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, 6, 6)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,406,310\n",
      "Trainable params: 1,406,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 4, 80, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 19, 19)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 64, 8, 8)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 6, 6)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,406,310\n",
      "Trainable params: 1,406,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0/1000, score: -21.0, e: 0.97, average: -21.00 SAVING\n",
      "episode: 1/1000, score: -21.0, e: 0.95, average: -21.00 SAVING\n",
      "episode: 2/1000, score: -20.0, e: 0.93, average: -20.67 SAVING\n",
      "episode: 3/1000, score: -19.0, e: 0.90, average: -20.25 SAVING\n",
      "episode: 4/1000, score: -21.0, e: 0.88, average: -20.40 \n",
      "episode: 5/1000, score: -20.0, e: 0.85, average: -20.33 \n",
      "episode: 6/1000, score: -20.0, e: 0.83, average: -20.29 \n",
      "episode: 7/1000, score: -20.0, e: 0.81, average: -20.25 SAVING\n",
      "episode: 8/1000, score: -21.0, e: 0.79, average: -20.33 \n",
      "episode: 9/1000, score: -19.0, e: 0.77, average: -20.20 SAVING\n",
      "episode: 10/1000, score: -21.0, e: 0.75, average: -20.27 \n",
      "episode: 11/1000, score: -21.0, e: 0.73, average: -20.33 \n",
      "episode: 12/1000, score: -20.0, e: 0.72, average: -20.31 \n",
      "episode: 13/1000, score: -21.0, e: 0.70, average: -20.36 \n",
      "episode: 14/1000, score: -18.0, e: 0.68, average: -20.20 SAVING\n",
      "episode: 15/1000, score: -18.0, e: 0.66, average: -20.06 SAVING\n",
      "episode: 16/1000, score: -21.0, e: 0.64, average: -20.12 \n",
      "episode: 17/1000, score: -19.0, e: 0.62, average: -20.06 SAVING\n",
      "episode: 18/1000, score: -21.0, e: 0.61, average: -20.11 \n",
      "episode: 19/1000, score: -19.0, e: 0.59, average: -20.05 SAVING\n",
      "episode: 20/1000, score: -21.0, e: 0.58, average: -20.10 \n",
      "episode: 21/1000, score: -21.0, e: 0.57, average: -20.14 \n",
      "episode: 22/1000, score: -19.0, e: 0.55, average: -20.09 \n",
      "episode: 23/1000, score: -21.0, e: 0.53, average: -20.12 \n",
      "episode: 24/1000, score: -21.0, e: 0.52, average: -20.16 \n",
      "episode: 25/1000, score: -19.0, e: 0.51, average: -20.12 \n",
      "episode: 26/1000, score: -20.0, e: 0.50, average: -20.11 \n",
      "episode: 27/1000, score: -21.0, e: 0.48, average: -20.14 \n",
      "episode: 28/1000, score: -21.0, e: 0.47, average: -20.17 \n",
      "episode: 29/1000, score: -21.0, e: 0.46, average: -20.20 \n",
      "episode: 30/1000, score: -21.0, e: 0.45, average: -20.23 \n",
      "episode: 31/1000, score: -21.0, e: 0.44, average: -20.25 \n",
      "episode: 32/1000, score: -21.0, e: 0.43, average: -20.27 \n",
      "episode: 33/1000, score: -21.0, e: 0.42, average: -20.29 \n",
      "episode: 34/1000, score: -21.0, e: 0.41, average: -20.31 \n",
      "episode: 35/1000, score: -21.0, e: 0.40, average: -20.33 \n",
      "episode: 36/1000, score: -20.0, e: 0.39, average: -20.32 \n",
      "episode: 37/1000, score: -20.0, e: 0.38, average: -20.32 \n",
      "episode: 38/1000, score: -20.0, e: 0.37, average: -20.31 \n",
      "episode: 39/1000, score: -21.0, e: 0.37, average: -20.32 \n",
      "episode: 40/1000, score: -17.0, e: 0.35, average: -20.24 \n",
      "episode: 41/1000, score: -21.0, e: 0.35, average: -20.26 \n",
      "episode: 42/1000, score: -20.0, e: 0.34, average: -20.26 \n",
      "episode: 43/1000, score: -21.0, e: 0.33, average: -20.27 \n",
      "episode: 44/1000, score: -21.0, e: 0.32, average: -20.29 \n",
      "episode: 45/1000, score: -21.0, e: 0.31, average: -20.30 \n",
      "episode: 46/1000, score: -20.0, e: 0.30, average: -20.30 \n",
      "episode: 47/1000, score: -18.0, e: 0.29, average: -20.25 \n",
      "episode: 48/1000, score: -19.0, e: 0.28, average: -20.22 \n",
      "episode: 49/1000, score: -20.0, e: 0.28, average: -20.22 \n",
      "episode: 50/1000, score: -20.0, e: 0.27, average: -20.20 \n",
      "episode: 51/1000, score: -21.0, e: 0.26, average: -20.20 \n",
      "episode: 52/1000, score: -21.0, e: 0.25, average: -20.22 \n",
      "episode: 53/1000, score: -20.0, e: 0.24, average: -20.24 \n",
      "episode: 54/1000, score: -21.0, e: 0.24, average: -20.24 \n",
      "episode: 55/1000, score: -21.0, e: 0.23, average: -20.26 \n",
      "episode: 56/1000, score: -21.0, e: 0.22, average: -20.28 \n",
      "episode: 57/1000, score: -19.0, e: 0.22, average: -20.26 \n",
      "episode: 58/1000, score: -21.0, e: 0.21, average: -20.26 \n",
      "episode: 59/1000, score: -20.0, e: 0.20, average: -20.28 \n",
      "episode: 60/1000, score: -19.0, e: 0.19, average: -20.24 \n",
      "episode: 61/1000, score: -19.0, e: 0.19, average: -20.20 \n",
      "episode: 62/1000, score: -17.0, e: 0.18, average: -20.14 \n",
      "episode: 63/1000, score: -19.0, e: 0.17, average: -20.10 \n",
      "episode: 64/1000, score: -15.0, e: 0.16, average: -20.04 SAVING\n",
      "episode: 65/1000, score: -17.0, e: 0.15, average: -20.02 SAVING\n",
      "episode: 66/1000, score: -18.0, e: 0.15, average: -19.96 SAVING\n",
      "episode: 67/1000, score: -17.0, e: 0.14, average: -19.92 SAVING\n",
      "episode: 68/1000, score: -18.0, e: 0.14, average: -19.86 SAVING\n",
      "episode: 69/1000, score: -15.0, e: 0.13, average: -19.78 SAVING\n",
      "episode: 70/1000, score: -18.0, e: 0.12, average: -19.72 SAVING\n",
      "episode: 71/1000, score: -19.0, e: 0.12, average: -19.68 SAVING\n",
      "episode: 72/1000, score: -19.0, e: 0.11, average: -19.68 SAVING\n",
      "episode: 73/1000, score: -15.0, e: 0.11, average: -19.56 SAVING\n",
      "episode: 74/1000, score: -15.0, e: 0.10, average: -19.44 SAVING\n",
      "episode: 75/1000, score: -15.0, e: 0.10, average: -19.36 SAVING\n",
      "episode: 76/1000, score: -19.0, e: 0.09, average: -19.34 SAVING\n",
      "episode: 77/1000, score: -16.0, e: 0.09, average: -19.24 SAVING\n",
      "episode: 78/1000, score: -15.0, e: 0.09, average: -19.12 SAVING\n",
      "episode: 79/1000, score: -20.0, e: 0.08, average: -19.10 SAVING\n",
      "episode: 80/1000, score: -16.0, e: 0.08, average: -19.00 SAVING\n",
      "episode: 81/1000, score: -15.0, e: 0.07, average: -18.88 SAVING\n",
      "episode: 82/1000, score: -18.0, e: 0.07, average: -18.82 SAVING\n",
      "episode: 83/1000, score: -16.0, e: 0.07, average: -18.72 SAVING\n",
      "episode: 84/1000, score: -19.0, e: 0.06, average: -18.68 SAVING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 85/1000, score: -14.0, e: 0.06, average: -18.54 SAVING\n",
      "episode: 86/1000, score: -18.0, e: 0.06, average: -18.50 SAVING\n",
      "episode: 87/1000, score: -18.0, e: 0.05, average: -18.46 SAVING\n",
      "episode: 88/1000, score: -18.0, e: 0.05, average: -18.42 SAVING\n",
      "episode: 89/1000, score: -16.0, e: 0.05, average: -18.32 SAVING\n",
      "episode: 90/1000, score: -17.0, e: 0.05, average: -18.32 SAVING\n",
      "episode: 91/1000, score: -9.0, e: 0.04, average: -18.08 SAVING\n",
      "episode: 92/1000, score: -18.0, e: 0.04, average: -18.04 SAVING\n",
      "episode: 93/1000, score: -19.0, e: 0.04, average: -18.00 SAVING\n",
      "episode: 94/1000, score: -16.0, e: 0.04, average: -17.90 SAVING\n",
      "episode: 95/1000, score: -15.0, e: 0.03, average: -17.78 SAVING\n",
      "episode: 96/1000, score: -15.0, e: 0.03, average: -17.68 SAVING\n",
      "episode: 97/1000, score: -12.0, e: 0.03, average: -17.56 SAVING\n",
      "episode: 98/1000, score: -11.0, e: 0.03, average: -17.40 SAVING\n",
      "episode: 99/1000, score: -15.0, e: 0.03, average: -17.30 SAVING\n",
      "episode: 100/1000, score: -13.0, e: 0.03, average: -17.16 SAVING\n",
      "episode: 101/1000, score: -15.0, e: 0.03, average: -17.04 SAVING\n",
      "episode: 102/1000, score: -10.0, e: 0.02, average: -16.82 SAVING\n",
      "episode: 103/1000, score: -12.0, e: 0.02, average: -16.66 SAVING\n",
      "episode: 104/1000, score: -7.0, e: 0.02, average: -16.38 SAVING\n",
      "episode: 105/1000, score: -11.0, e: 0.02, average: -16.18 SAVING\n",
      "episode: 106/1000, score: -13.0, e: 0.02, average: -16.02 SAVING\n",
      "episode: 107/1000, score: -14.0, e: 0.02, average: -15.92 SAVING\n",
      "episode: 108/1000, score: -13.0, e: 0.02, average: -15.76 SAVING\n",
      "episode: 109/1000, score: -10.0, e: 0.02, average: -15.56 SAVING\n",
      "episode: 110/1000, score: -10.0, e: 0.02, average: -15.38 SAVING\n",
      "episode: 111/1000, score: -9.0, e: 0.02, average: -15.18 SAVING\n",
      "episode: 112/1000, score: -9.0, e: 0.02, average: -15.02 SAVING\n",
      "episode: 113/1000, score: -8.0, e: 0.02, average: -14.80 SAVING\n",
      "episode: 114/1000, score: -16.0, e: 0.02, average: -14.82 \n",
      "episode: 115/1000, score: -13.0, e: 0.02, average: -14.74 SAVING\n",
      "episode: 116/1000, score: -14.0, e: 0.02, average: -14.66 SAVING\n",
      "episode: 117/1000, score: -11.0, e: 0.02, average: -14.54 SAVING\n",
      "episode: 118/1000, score: -7.0, e: 0.02, average: -14.32 SAVING\n",
      "episode: 119/1000, score: -8.0, e: 0.02, average: -14.18 SAVING\n",
      "episode: 120/1000, score: -10.0, e: 0.02, average: -14.02 SAVING\n",
      "episode: 121/1000, score: -9.0, e: 0.02, average: -13.82 SAVING\n",
      "episode: 122/1000, score: -3.0, e: 0.02, average: -13.50 SAVING\n",
      "episode: 123/1000, score: -11.0, e: 0.02, average: -13.42 SAVING\n",
      "episode: 124/1000, score: -11.0, e: 0.02, average: -13.34 SAVING\n",
      "episode: 125/1000, score: -12.0, e: 0.02, average: -13.28 SAVING\n",
      "episode: 126/1000, score: -9.0, e: 0.02, average: -13.08 SAVING\n",
      "episode: 127/1000, score: -8.0, e: 0.02, average: -12.92 SAVING\n",
      "episode: 128/1000, score: -17.0, e: 0.02, average: -12.96 \n",
      "episode: 129/1000, score: -13.0, e: 0.02, average: -12.82 SAVING\n",
      "episode: 130/1000, score: -14.0, e: 0.02, average: -12.78 SAVING\n",
      "episode: 131/1000, score: -12.0, e: 0.02, average: -12.72 SAVING\n",
      "episode: 132/1000, score: -5.0, e: 0.02, average: -12.46 SAVING\n",
      "episode: 133/1000, score: -8.0, e: 0.02, average: -12.30 SAVING\n",
      "episode: 134/1000, score: -14.0, e: 0.02, average: -12.20 SAVING\n",
      "episode: 135/1000, score: -12.0, e: 0.02, average: -12.16 SAVING\n",
      "episode: 136/1000, score: -13.0, e: 0.02, average: -12.06 SAVING\n",
      "episode: 137/1000, score: -14.0, e: 0.02, average: -11.98 SAVING\n",
      "episode: 138/1000, score: -8.0, e: 0.02, average: -11.78 SAVING\n",
      "episode: 139/1000, score: -8.0, e: 0.02, average: -11.62 SAVING\n",
      "episode: 140/1000, score: -8.0, e: 0.02, average: -11.44 SAVING\n",
      "episode: 141/1000, score: -13.0, e: 0.02, average: -11.52 \n",
      "episode: 142/1000, score: -12.0, e: 0.02, average: -11.40 SAVING\n",
      "episode: 143/1000, score: -16.0, e: 0.02, average: -11.34 SAVING\n",
      "episode: 144/1000, score: -15.0, e: 0.02, average: -11.32 SAVING\n",
      "episode: 145/1000, score: -11.0, e: 0.02, average: -11.24 SAVING\n",
      "episode: 146/1000, score: -7.0, e: 0.02, average: -11.08 SAVING\n",
      "episode: 147/1000, score: -13.0, e: 0.02, average: -11.10 \n",
      "episode: 148/1000, score: -13.0, e: 0.02, average: -11.14 \n",
      "episode: 149/1000, score: -15.0, e: 0.02, average: -11.14 \n",
      "episode: 150/1000, score: -6.0, e: 0.02, average: -11.00 SAVING\n",
      "episode: 151/1000, score: -7.0, e: 0.02, average: -10.84 SAVING\n",
      "episode: 152/1000, score: -15.0, e: 0.02, average: -10.94 \n",
      "episode: 153/1000, score: -15.0, e: 0.02, average: -11.00 \n",
      "episode: 154/1000, score: -12.0, e: 0.02, average: -11.10 \n",
      "episode: 155/1000, score: -18.0, e: 0.02, average: -11.24 \n",
      "episode: 156/1000, score: 1.0, e: 0.02, average: -10.96 \n",
      "episode: 157/1000, score: -9.0, e: 0.02, average: -10.86 \n",
      "episode: 158/1000, score: -13.0, e: 0.02, average: -10.86 \n",
      "episode: 159/1000, score: -15.0, e: 0.02, average: -10.96 \n",
      "episode: 160/1000, score: -4.0, e: 0.02, average: -10.84 SAVING\n",
      "episode: 161/1000, score: -13.0, e: 0.02, average: -10.92 \n",
      "episode: 162/1000, score: -11.0, e: 0.02, average: -10.96 \n",
      "episode: 163/1000, score: -4.0, e: 0.02, average: -10.88 \n",
      "episode: 164/1000, score: -11.0, e: 0.02, average: -10.78 SAVING\n",
      "episode: 165/1000, score: -9.0, e: 0.02, average: -10.70 SAVING\n",
      "episode: 166/1000, score: -8.0, e: 0.02, average: -10.58 SAVING\n",
      "episode: 167/1000, score: 3.0, e: 0.02, average: -10.30 SAVING\n",
      "episode: 168/1000, score: -8.0, e: 0.02, average: -10.32 \n",
      "episode: 169/1000, score: -2.0, e: 0.02, average: -10.20 SAVING\n",
      "episode: 170/1000, score: -6.0, e: 0.02, average: -10.12 SAVING\n",
      "episode: 171/1000, score: -6.0, e: 0.02, average: -10.06 SAVING\n",
      "episode: 172/1000, score: -12.0, e: 0.02, average: -10.24 \n",
      "episode: 173/1000, score: -14.0, e: 0.02, average: -10.30 \n",
      "episode: 174/1000, score: -14.0, e: 0.02, average: -10.36 \n",
      "episode: 175/1000, score: -3.0, e: 0.02, average: -10.18 \n",
      "episode: 176/1000, score: -8.0, e: 0.02, average: -10.16 \n",
      "episode: 177/1000, score: -6.0, e: 0.02, average: -10.12 \n",
      "episode: 178/1000, score: 1.0, e: 0.02, average: -9.76 SAVING\n",
      "episode: 179/1000, score: -7.0, e: 0.02, average: -9.64 SAVING\n",
      "episode: 180/1000, score: -14.0, e: 0.02, average: -9.64 SAVING\n",
      "episode: 181/1000, score: -1.0, e: 0.02, average: -9.42 SAVING\n",
      "episode: 182/1000, score: -8.0, e: 0.02, average: -9.48 \n",
      "episode: 183/1000, score: -8.0, e: 0.02, average: -9.48 \n",
      "episode: 184/1000, score: -6.0, e: 0.02, average: -9.32 SAVING\n",
      "episode: 185/1000, score: -14.0, e: 0.02, average: -9.36 \n",
      "episode: 186/1000, score: -10.0, e: 0.02, average: -9.30 SAVING\n",
      "episode: 187/1000, score: -13.0, e: 0.02, average: -9.28 SAVING\n",
      "episode: 188/1000, score: 2.0, e: 0.02, average: -9.08 SAVING\n",
      "episode: 189/1000, score: -12.0, e: 0.02, average: -9.16 \n",
      "episode: 190/1000, score: -15.0, e: 0.02, average: -9.30 \n",
      "episode: 191/1000, score: -5.0, e: 0.02, average: -9.14 \n",
      "episode: 192/1000, score: -6.0, e: 0.02, average: -9.02 SAVING\n",
      "episode: 193/1000, score: -11.0, e: 0.02, average: -8.92 SAVING\n",
      "episode: 194/1000, score: -14.0, e: 0.02, average: -8.90 SAVING\n",
      "episode: 195/1000, score: -4.0, e: 0.02, average: -8.76 SAVING\n",
      "episode: 196/1000, score: -9.0, e: 0.02, average: -8.80 \n",
      "episode: 197/1000, score: -9.0, e: 0.02, average: -8.72 SAVING\n",
      "episode: 198/1000, score: -13.0, e: 0.02, average: -8.72 SAVING\n",
      "episode: 199/1000, score: -12.0, e: 0.02, average: -8.66 SAVING\n",
      "episode: 200/1000, score: -12.0, e: 0.02, average: -8.78 \n",
      "episode: 201/1000, score: -10.0, e: 0.02, average: -8.84 \n",
      "episode: 202/1000, score: -3.0, e: 0.02, average: -8.60 SAVING\n",
      "episode: 203/1000, score: -12.0, e: 0.02, average: -8.54 SAVING\n",
      "episode: 204/1000, score: -7.0, e: 0.02, average: -8.44 SAVING\n",
      "episode: 205/1000, score: -12.0, e: 0.02, average: -8.32 SAVING\n",
      "episode: 206/1000, score: -11.0, e: 0.02, average: -8.56 \n",
      "episode: 207/1000, score: -8.0, e: 0.02, average: -8.54 \n",
      "episode: 208/1000, score: 4.0, e: 0.02, average: -8.20 SAVING\n",
      "episode: 209/1000, score: -4.0, e: 0.02, average: -7.98 SAVING\n",
      "episode: 210/1000, score: -5.0, e: 0.02, average: -8.00 \n",
      "episode: 211/1000, score: -8.0, e: 0.02, average: -7.90 SAVING\n",
      "episode: 212/1000, score: -8.0, e: 0.02, average: -7.84 SAVING\n",
      "episode: 213/1000, score: -2.0, e: 0.02, average: -7.80 SAVING\n",
      "episode: 214/1000, score: -4.0, e: 0.02, average: -7.66 SAVING\n",
      "episode: 215/1000, score: -14.0, e: 0.02, average: -7.76 \n",
      "episode: 216/1000, score: 4.0, e: 0.02, average: -7.52 SAVING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 217/1000, score: 8.0, e: 0.02, average: -7.42 SAVING\n",
      "episode: 218/1000, score: -12.0, e: 0.02, average: -7.50 \n",
      "episode: 219/1000, score: -5.0, e: 0.02, average: -7.56 \n",
      "episode: 220/1000, score: -8.0, e: 0.02, average: -7.60 \n",
      "episode: 221/1000, score: -7.0, e: 0.02, average: -7.62 \n",
      "episode: 222/1000, score: -11.0, e: 0.02, average: -7.60 \n",
      "episode: 223/1000, score: -9.0, e: 0.02, average: -7.50 \n",
      "episode: 224/1000, score: -9.0, e: 0.02, average: -7.40 SAVING\n",
      "episode: 225/1000, score: 2.0, e: 0.02, average: -7.30 SAVING\n",
      "episode: 226/1000, score: -11.0, e: 0.02, average: -7.36 \n",
      "episode: 227/1000, score: -8.0, e: 0.02, average: -7.40 \n",
      "episode: 228/1000, score: 1.0, e: 0.02, average: -7.40 \n",
      "episode: 229/1000, score: -3.0, e: 0.02, average: -7.32 \n",
      "episode: 230/1000, score: 1.0, e: 0.02, average: -7.02 SAVING\n",
      "episode: 231/1000, score: -1.0, e: 0.02, average: -7.02 SAVING\n",
      "episode: 232/1000, score: -8.0, e: 0.02, average: -7.02 SAVING\n",
      "episode: 233/1000, score: 1.0, e: 0.02, average: -6.84 SAVING\n",
      "episode: 234/1000, score: -2.0, e: 0.02, average: -6.76 SAVING\n",
      "episode: 235/1000, score: -5.0, e: 0.02, average: -6.58 SAVING\n",
      "episode: 236/1000, score: 3.0, e: 0.02, average: -6.32 SAVING\n",
      "episode: 237/1000, score: -6.0, e: 0.02, average: -6.18 SAVING\n",
      "episode: 238/1000, score: -2.0, e: 0.02, average: -6.26 \n",
      "episode: 239/1000, score: -1.0, e: 0.02, average: -6.04 SAVING\n",
      "episode: 240/1000, score: -4.0, e: 0.02, average: -5.82 SAVING\n",
      "episode: 241/1000, score: -3.0, e: 0.02, average: -5.78 SAVING\n",
      "episode: 242/1000, score: 1.0, e: 0.02, average: -5.64 SAVING\n",
      "episode: 243/1000, score: -7.0, e: 0.02, average: -5.56 SAVING\n",
      "episode: 244/1000, score: 4.0, e: 0.02, average: -5.20 SAVING\n",
      "episode: 245/1000, score: -1.0, e: 0.02, average: -5.14 SAVING\n",
      "episode: 246/1000, score: -1.0, e: 0.02, average: -4.98 SAVING\n",
      "episode: 247/1000, score: 2.0, e: 0.02, average: -4.76 SAVING\n",
      "episode: 248/1000, score: -6.0, e: 0.02, average: -4.62 SAVING\n",
      "episode: 249/1000, score: 3.0, e: 0.02, average: -4.32 SAVING\n",
      "episode: 250/1000, score: 9.0, e: 0.02, average: -3.90 SAVING\n",
      "episode: 251/1000, score: -17.0, e: 0.02, average: -4.04 \n",
      "episode: 252/1000, score: -1.0, e: 0.02, average: -4.00 \n",
      "episode: 253/1000, score: -11.0, e: 0.02, average: -3.98 \n",
      "episode: 254/1000, score: -17.0, e: 0.02, average: -4.18 \n",
      "episode: 255/1000, score: -8.0, e: 0.02, average: -4.10 \n",
      "episode: 256/1000, score: -4.0, e: 0.02, average: -3.96 \n",
      "episode: 257/1000, score: -4.0, e: 0.02, average: -3.88 SAVING\n",
      "episode: 258/1000, score: 1.0, e: 0.02, average: -3.94 \n",
      "episode: 259/1000, score: -3.0, e: 0.02, average: -3.92 \n",
      "episode: 260/1000, score: -3.0, e: 0.02, average: -3.88 SAVING\n",
      "episode: 261/1000, score: -4.0, e: 0.02, average: -3.80 SAVING\n",
      "episode: 262/1000, score: 1.0, e: 0.02, average: -3.62 SAVING\n",
      "episode: 263/1000, score: 4.0, e: 0.02, average: -3.50 SAVING\n",
      "episode: 264/1000, score: -3.0, e: 0.02, average: -3.48 SAVING\n",
      "episode: 265/1000, score: 8.0, e: 0.02, average: -3.04 SAVING\n",
      "episode: 266/1000, score: 1.0, e: 0.02, average: -3.10 \n",
      "episode: 267/1000, score: -12.0, e: 0.02, average: -3.50 \n",
      "episode: 268/1000, score: -17.0, e: 0.02, average: -3.60 \n",
      "episode: 269/1000, score: -5.0, e: 0.02, average: -3.60 \n",
      "episode: 270/1000, score: -11.0, e: 0.02, average: -3.66 \n",
      "episode: 271/1000, score: -2.0, e: 0.02, average: -3.56 \n",
      "episode: 272/1000, score: -9.0, e: 0.02, average: -3.52 \n",
      "episode: 273/1000, score: -13.0, e: 0.02, average: -3.60 \n",
      "episode: 274/1000, score: -4.0, e: 0.02, average: -3.50 \n",
      "episode: 275/1000, score: -3.0, e: 0.02, average: -3.60 \n",
      "episode: 276/1000, score: -3.0, e: 0.02, average: -3.44 \n",
      "episode: 277/1000, score: -11.0, e: 0.02, average: -3.50 \n",
      "episode: 278/1000, score: -14.0, e: 0.02, average: -3.80 \n",
      "episode: 279/1000, score: -2.0, e: 0.02, average: -3.78 \n",
      "episode: 280/1000, score: -7.0, e: 0.02, average: -3.94 \n",
      "episode: 281/1000, score: -10.0, e: 0.02, average: -4.12 \n",
      "episode: 282/1000, score: -4.0, e: 0.02, average: -4.04 \n",
      "episode: 283/1000, score: -3.0, e: 0.02, average: -4.12 \n",
      "episode: 284/1000, score: -13.0, e: 0.02, average: -4.34 \n",
      "episode: 285/1000, score: 8.0, e: 0.02, average: -4.08 \n",
      "episode: 286/1000, score: 5.0, e: 0.02, average: -4.04 \n",
      "episode: 287/1000, score: -2.0, e: 0.02, average: -3.96 \n",
      "episode: 288/1000, score: -7.0, e: 0.02, average: -4.06 \n",
      "episode: 289/1000, score: -2.0, e: 0.02, average: -4.08 \n",
      "episode: 290/1000, score: -7.0, e: 0.02, average: -4.14 \n",
      "episode: 291/1000, score: 8.0, e: 0.02, average: -3.92 \n",
      "episode: 292/1000, score: -7.0, e: 0.02, average: -4.08 \n",
      "episode: 293/1000, score: -2.0, e: 0.02, average: -3.98 \n",
      "episode: 294/1000, score: -7.0, e: 0.02, average: -4.20 \n",
      "episode: 295/1000, score: -9.0, e: 0.02, average: -4.36 \n",
      "episode: 296/1000, score: -11.0, e: 0.02, average: -4.56 \n",
      "episode: 297/1000, score: -8.0, e: 0.02, average: -4.76 \n",
      "episode: 298/1000, score: -3.0, e: 0.02, average: -4.70 \n",
      "episode: 299/1000, score: 5.0, e: 0.02, average: -4.66 \n",
      "episode: 300/1000, score: -9.0, e: 0.02, average: -5.02 \n",
      "episode: 301/1000, score: 7.0, e: 0.02, average: -4.54 \n",
      "episode: 302/1000, score: -9.0, e: 0.02, average: -4.70 \n",
      "episode: 303/1000, score: -3.0, e: 0.02, average: -4.54 \n",
      "episode: 304/1000, score: -6.0, e: 0.02, average: -4.32 \n",
      "episode: 305/1000, score: 9.0, e: 0.02, average: -3.98 \n",
      "episode: 306/1000, score: 3.0, e: 0.02, average: -3.84 \n",
      "episode: 307/1000, score: -5.0, e: 0.02, average: -3.86 \n",
      "episode: 308/1000, score: -9.0, e: 0.02, average: -4.06 \n",
      "episode: 309/1000, score: -9.0, e: 0.02, average: -4.18 \n",
      "episode: 310/1000, score: 8.0, e: 0.02, average: -3.96 \n",
      "episode: 311/1000, score: 6.0, e: 0.02, average: -3.76 \n",
      "episode: 312/1000, score: 13.0, e: 0.02, average: -3.52 \n",
      "episode: 313/1000, score: -1.0, e: 0.02, average: -3.62 \n",
      "episode: 314/1000, score: 1.0, e: 0.02, average: -3.54 \n",
      "episode: 315/1000, score: -6.0, e: 0.02, average: -3.82 \n",
      "episode: 316/1000, score: -13.0, e: 0.02, average: -4.10 \n",
      "episode: 317/1000, score: -4.0, e: 0.02, average: -3.94 \n",
      "episode: 318/1000, score: -3.0, e: 0.02, average: -3.66 \n",
      "episode: 319/1000, score: -10.0, e: 0.02, average: -3.76 \n",
      "episode: 320/1000, score: 4.0, e: 0.02, average: -3.46 \n",
      "episode: 321/1000, score: -4.0, e: 0.02, average: -3.50 \n",
      "episode: 322/1000, score: -7.0, e: 0.02, average: -3.46 \n",
      "episode: 323/1000, score: -9.0, e: 0.02, average: -3.38 \n",
      "episode: 324/1000, score: 2.0, e: 0.02, average: -3.26 \n",
      "episode: 325/1000, score: -4.0, e: 0.02, average: -3.28 \n",
      "episode: 326/1000, score: -12.0, e: 0.02, average: -3.46 \n",
      "episode: 327/1000, score: 14.0, e: 0.02, average: -2.96 SAVING\n",
      "episode: 328/1000, score: -11.0, e: 0.02, average: -2.90 SAVING\n",
      "episode: 329/1000, score: -8.0, e: 0.02, average: -3.02 \n",
      "episode: 330/1000, score: -12.0, e: 0.02, average: -3.12 \n",
      "episode: 331/1000, score: -10.0, e: 0.02, average: -3.12 \n",
      "episode: 332/1000, score: 3.0, e: 0.02, average: -2.98 \n",
      "episode: 333/1000, score: 4.0, e: 0.02, average: -2.84 SAVING\n",
      "episode: 334/1000, score: -9.0, e: 0.02, average: -2.76 SAVING\n",
      "episode: 335/1000, score: -1.0, e: 0.02, average: -2.94 \n",
      "episode: 336/1000, score: 9.0, e: 0.02, average: -2.86 \n",
      "episode: 337/1000, score: -7.0, e: 0.02, average: -2.96 \n",
      "episode: 338/1000, score: -1.0, e: 0.02, average: -2.84 \n",
      "episode: 339/1000, score: -10.0, e: 0.02, average: -3.00 \n",
      "episode: 340/1000, score: 10.0, e: 0.02, average: -2.66 SAVING\n",
      "episode: 341/1000, score: -10.0, e: 0.02, average: -3.02 \n",
      "episode: 342/1000, score: -13.0, e: 0.02, average: -3.14 \n",
      "episode: 343/1000, score: -10.0, e: 0.02, average: -3.30 \n",
      "episode: 344/1000, score: -2.0, e: 0.02, average: -3.20 \n",
      "episode: 345/1000, score: -11.0, e: 0.02, average: -3.24 \n",
      "episode: 346/1000, score: -13.0, e: 0.02, average: -3.28 \n",
      "episode: 347/1000, score: -7.0, e: 0.02, average: -3.26 \n",
      "episode: 348/1000, score: -11.0, e: 0.02, average: -3.42 \n",
      "episode: 349/1000, score: 1.0, e: 0.02, average: -3.50 \n",
      "episode: 350/1000, score: -5.0, e: 0.02, average: -3.42 \n",
      "episode: 351/1000, score: 2.0, e: 0.02, average: -3.52 \n",
      "episode: 352/1000, score: -7.0, e: 0.02, average: -3.48 \n",
      "episode: 353/1000, score: -3.0, e: 0.02, average: -3.48 \n",
      "episode: 354/1000, score: -8.0, e: 0.02, average: -3.52 \n",
      "episode: 355/1000, score: -3.0, e: 0.02, average: -3.76 \n",
      "episode: 356/1000, score: 1.0, e: 0.02, average: -3.80 \n",
      "episode: 357/1000, score: -2.0, e: 0.02, average: -3.74 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 358/1000, score: -1.0, e: 0.02, average: -3.58 \n",
      "episode: 359/1000, score: -17.0, e: 0.02, average: -3.74 \n",
      "episode: 360/1000, score: -2.0, e: 0.02, average: -3.94 \n",
      "episode: 361/1000, score: -6.0, e: 0.02, average: -4.18 \n",
      "episode: 362/1000, score: -5.0, e: 0.02, average: -4.54 \n",
      "episode: 363/1000, score: -4.0, e: 0.02, average: -4.60 \n",
      "episode: 364/1000, score: -6.0, e: 0.02, average: -4.74 \n",
      "episode: 365/1000, score: -12.0, e: 0.02, average: -4.86 \n",
      "episode: 366/1000, score: -15.0, e: 0.02, average: -4.90 \n",
      "episode: 367/1000, score: -5.0, e: 0.02, average: -4.92 \n",
      "episode: 368/1000, score: -6.0, e: 0.02, average: -4.98 \n",
      "episode: 369/1000, score: 5.0, e: 0.02, average: -4.68 \n",
      "episode: 370/1000, score: -5.0, e: 0.02, average: -4.86 \n",
      "episode: 371/1000, score: -7.0, e: 0.02, average: -4.92 \n",
      "episode: 372/1000, score: -7.0, e: 0.02, average: -4.92 \n",
      "episode: 373/1000, score: -3.0, e: 0.02, average: -4.80 \n",
      "episode: 374/1000, score: 3.0, e: 0.02, average: -4.78 \n",
      "episode: 375/1000, score: -3.0, e: 0.02, average: -4.76 \n",
      "episode: 376/1000, score: -6.0, e: 0.02, average: -4.64 \n",
      "episode: 377/1000, score: -10.0, e: 0.02, average: -5.12 \n",
      "episode: 378/1000, score: -7.0, e: 0.02, average: -5.04 \n",
      "episode: 379/1000, score: -3.0, e: 0.02, average: -4.94 \n",
      "episode: 380/1000, score: -8.0, e: 0.02, average: -4.86 \n",
      "episode: 381/1000, score: -9.0, e: 0.02, average: -4.84 \n",
      "episode: 382/1000, score: -8.0, e: 0.02, average: -5.06 \n",
      "episode: 383/1000, score: -11.0, e: 0.02, average: -5.36 \n",
      "episode: 384/1000, score: 4.0, e: 0.02, average: -5.10 \n",
      "episode: 385/1000, score: -8.0, e: 0.02, average: -5.24 \n",
      "episode: 386/1000, score: -9.0, e: 0.02, average: -5.60 \n",
      "episode: 387/1000, score: -11.0, e: 0.02, average: -5.68 \n",
      "episode: 388/1000, score: -11.0, e: 0.02, average: -5.88 \n",
      "episode: 389/1000, score: -10.0, e: 0.02, average: -5.88 \n",
      "episode: 390/1000, score: -5.0, e: 0.02, average: -6.18 \n",
      "episode: 391/1000, score: -11.0, e: 0.02, average: -6.20 \n",
      "episode: 392/1000, score: -8.0, e: 0.02, average: -6.10 \n",
      "episode: 393/1000, score: -5.0, e: 0.02, average: -6.00 \n",
      "episode: 394/1000, score: -7.0, e: 0.02, average: -6.10 \n",
      "episode: 395/1000, score: -8.0, e: 0.02, average: -6.04 \n",
      "episode: 396/1000, score: -11.0, e: 0.02, average: -6.00 \n",
      "episode: 397/1000, score: -7.0, e: 0.02, average: -6.00 \n",
      "episode: 398/1000, score: -6.0, e: 0.02, average: -5.90 \n",
      "episode: 399/1000, score: -7.0, e: 0.02, average: -6.06 \n",
      "episode: 400/1000, score: -5.0, e: 0.02, average: -6.06 \n",
      "episode: 401/1000, score: -7.0, e: 0.02, average: -6.24 \n",
      "episode: 402/1000, score: 2.0, e: 0.02, average: -6.06 \n",
      "episode: 403/1000, score: -12.0, e: 0.02, average: -6.24 \n",
      "episode: 404/1000, score: 3.0, e: 0.02, average: -6.02 \n",
      "episode: 405/1000, score: -9.0, e: 0.02, average: -6.14 \n",
      "episode: 406/1000, score: -8.0, e: 0.02, average: -6.32 \n",
      "episode: 407/1000, score: -10.0, e: 0.02, average: -6.48 \n",
      "episode: 408/1000, score: -2.0, e: 0.02, average: -6.50 \n",
      "episode: 409/1000, score: -12.0, e: 0.02, average: -6.40 \n",
      "episode: 410/1000, score: -14.0, e: 0.02, average: -6.64 \n",
      "episode: 411/1000, score: -7.0, e: 0.02, average: -6.66 \n",
      "episode: 412/1000, score: -9.0, e: 0.02, average: -6.74 \n",
      "episode: 413/1000, score: -6.0, e: 0.02, average: -6.78 \n",
      "episode: 414/1000, score: 6.0, e: 0.02, average: -6.54 \n",
      "episode: 415/1000, score: -6.0, e: 0.02, average: -6.42 \n",
      "episode: 416/1000, score: -4.0, e: 0.02, average: -6.20 \n",
      "episode: 417/1000, score: 1.0, e: 0.02, average: -6.08 \n",
      "episode: 418/1000, score: -18.0, e: 0.02, average: -6.32 \n",
      "episode: 419/1000, score: -6.0, e: 0.02, average: -6.54 \n",
      "episode: 420/1000, score: 2.0, e: 0.02, average: -6.40 \n",
      "episode: 421/1000, score: -16.0, e: 0.02, average: -6.58 \n",
      "episode: 422/1000, score: -13.0, e: 0.02, average: -6.70 \n",
      "episode: 423/1000, score: -12.0, e: 0.02, average: -6.88 \n",
      "episode: 424/1000, score: -13.0, e: 0.02, average: -7.20 \n",
      "episode: 425/1000, score: -14.0, e: 0.02, average: -7.42 \n",
      "episode: 426/1000, score: 3.0, e: 0.02, average: -7.24 \n",
      "episode: 427/1000, score: -9.0, e: 0.02, average: -7.22 \n",
      "episode: 428/1000, score: -9.0, e: 0.02, average: -7.26 \n",
      "episode: 429/1000, score: -5.0, e: 0.02, average: -7.30 \n",
      "episode: 430/1000, score: -13.0, e: 0.02, average: -7.40 \n",
      "episode: 431/1000, score: -6.0, e: 0.02, average: -7.34 \n",
      "episode: 432/1000, score: -13.0, e: 0.02, average: -7.44 \n",
      "episode: 433/1000, score: -14.0, e: 0.02, average: -7.50 \n",
      "episode: 434/1000, score: -11.0, e: 0.02, average: -7.80 \n",
      "episode: 435/1000, score: -17.0, e: 0.02, average: -7.98 \n",
      "episode: 436/1000, score: -7.0, e: 0.02, average: -7.94 \n",
      "episode: 437/1000, score: -3.0, e: 0.02, average: -7.78 \n",
      "episode: 438/1000, score: -9.0, e: 0.02, average: -7.74 \n",
      "episode: 439/1000, score: -9.0, e: 0.02, average: -7.72 \n",
      "episode: 440/1000, score: -9.0, e: 0.02, average: -7.80 \n",
      "episode: 441/1000, score: -11.0, e: 0.02, average: -7.80 \n",
      "episode: 442/1000, score: -7.0, e: 0.02, average: -7.78 \n",
      "episode: 443/1000, score: -7.0, e: 0.02, average: -7.82 \n",
      "episode: 444/1000, score: -11.0, e: 0.02, average: -7.90 \n",
      "episode: 445/1000, score: -6.0, e: 0.02, average: -7.86 \n",
      "episode: 446/1000, score: -12.0, e: 0.02, average: -7.88 \n",
      "episode: 447/1000, score: -5.0, e: 0.02, average: -7.84 \n",
      "episode: 448/1000, score: -10.0, e: 0.02, average: -7.92 \n",
      "episode: 449/1000, score: 5.0, e: 0.02, average: -7.68 \n",
      "episode: 450/1000, score: -8.0, e: 0.02, average: -7.74 \n",
      "episode: 451/1000, score: -14.0, e: 0.02, average: -7.88 \n",
      "episode: 452/1000, score: -8.0, e: 0.02, average: -8.08 \n",
      "episode: 453/1000, score: -5.0, e: 0.02, average: -7.94 \n",
      "episode: 454/1000, score: -9.0, e: 0.02, average: -8.18 \n",
      "episode: 455/1000, score: 3.0, e: 0.02, average: -7.94 \n",
      "episode: 456/1000, score: -9.0, e: 0.02, average: -7.96 \n",
      "episode: 457/1000, score: -6.0, e: 0.02, average: -7.88 \n",
      "episode: 458/1000, score: -6.0, e: 0.02, average: -7.96 \n",
      "episode: 459/1000, score: -6.0, e: 0.02, average: -7.84 \n",
      "episode: 460/1000, score: -10.0, e: 0.02, average: -7.76 \n",
      "episode: 461/1000, score: -5.0, e: 0.02, average: -7.72 \n",
      "episode: 462/1000, score: -11.0, e: 0.02, average: -7.76 \n",
      "episode: 463/1000, score: -8.0, e: 0.02, average: -7.80 \n",
      "episode: 464/1000, score: 3.0, e: 0.02, average: -7.86 \n",
      "episode: 465/1000, score: -5.0, e: 0.02, average: -7.84 \n",
      "episode: 466/1000, score: -19.0, e: 0.02, average: -8.14 \n",
      "episode: 467/1000, score: -10.0, e: 0.02, average: -8.36 \n",
      "episode: 468/1000, score: -15.0, e: 0.02, average: -8.30 \n",
      "episode: 469/1000, score: -14.0, e: 0.02, average: -8.46 \n",
      "episode: 470/1000, score: -11.0, e: 0.02, average: -8.72 \n",
      "episode: 471/1000, score: -10.0, e: 0.02, average: -8.60 \n",
      "episode: 472/1000, score: -15.0, e: 0.02, average: -8.64 \n",
      "episode: 473/1000, score: -9.0, e: 0.02, average: -8.58 \n",
      "episode: 474/1000, score: 3.0, e: 0.02, average: -8.26 \n",
      "episode: 475/1000, score: -4.0, e: 0.02, average: -8.06 \n",
      "episode: 476/1000, score: -15.0, e: 0.02, average: -8.42 \n",
      "episode: 477/1000, score: -13.0, e: 0.02, average: -8.50 \n",
      "episode: 478/1000, score: -1.0, e: 0.02, average: -8.34 \n",
      "episode: 479/1000, score: -7.0, e: 0.02, average: -8.38 \n",
      "episode: 480/1000, score: -11.0, e: 0.02, average: -8.34 \n",
      "episode: 481/1000, score: -9.0, e: 0.02, average: -8.40 \n",
      "episode: 482/1000, score: -5.0, e: 0.02, average: -8.24 \n",
      "episode: 483/1000, score: -9.0, e: 0.02, average: -8.14 \n",
      "episode: 484/1000, score: -12.0, e: 0.02, average: -8.16 \n",
      "episode: 485/1000, score: -7.0, e: 0.02, average: -7.96 \n",
      "episode: 486/1000, score: -8.0, e: 0.02, average: -7.98 \n",
      "episode: 487/1000, score: -13.0, e: 0.02, average: -8.18 \n",
      "episode: 488/1000, score: -11.0, e: 0.02, average: -8.22 \n",
      "episode: 489/1000, score: -3.0, e: 0.02, average: -8.10 \n",
      "episode: 490/1000, score: -11.0, e: 0.02, average: -8.14 \n",
      "episode: 491/1000, score: -5.0, e: 0.02, average: -8.02 \n",
      "episode: 492/1000, score: -1.0, e: 0.02, average: -7.90 \n"
     ]
    }
   ],
   "source": [
    "# Tutorial by www.pylessons.com\n",
    "# Tutorial written for - Tensorflow 1.15, Keras 2.2.4\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "\n",
    "def OurModel(input_shape, action_space, dueling):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    #X = Conv2D(64, 5, strides=(3, 3),padding=\"valid\", input_shape=input_shape, activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(32, 8, strides=(4, 4),padding=\"valid\", input_shape=input_shape, activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(64, 4, strides=(2, 2),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(64, 3, strides=(1, 1),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Flatten()(X)\n",
    "    \n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    X = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    if dueling:\n",
    "        state_value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "        state_value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(action_space,))(state_value)\n",
    "\n",
    "        action_advantage = Dense(action_space, kernel_initializer='he_uniform')(X)\n",
    "        action_advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(action_space,))(action_advantage)\n",
    "\n",
    "        X = Add()([state_value, action_advantage])\n",
    "    else:\n",
    "        # Output Layer with # of actions: 2 nodes (left, right)\n",
    "        X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    #model.compile(loss=\"mean_squared_error\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "    #model.compile(optimizer=Adam(lr=0.00025), loss='mean_squared_error')\n",
    "    model.compile(optimizer=Adam(lr=0.00005), loss='mean_squared_error')\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env_name):\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        #self.env.seed(0)  \n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        \n",
    "        # Instantiate memory\n",
    "        memory_size = 25000\n",
    "        self.MEMORY = Memory(memory_size)\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        \n",
    "        # EXPLORATION HYPERPARAMETERS for epsilon and epsilon greedy strategy\n",
    "        self.epsilon = 1.0  # exploration probability at start\n",
    "        self.epsilon_min = 0.02  # minimum exploration probability \n",
    "        self.epsilon_decay = 0.00002  # exponential decay rate for exploration prob\n",
    "        \n",
    "        self.batch_size = 32\n",
    "\n",
    "        # defining model parameters\n",
    "        self.ddqn = False # use doudle deep q network\n",
    "        self.dueling = False # use dealing netowrk\n",
    "        self.epsilon_greedy = False # use epsilon greedy strategy\n",
    "        self.USE_PER = False # use priority experienced replay\n",
    "        \n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name+\"_CNN.h5\")\n",
    "\n",
    "        self.ROWS = 80\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 4\n",
    "        self.update_model_steps = 1000\n",
    "        \n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "        \n",
    "        # create main model and target model\n",
    "        self.model = OurModel(input_shape=self.state_size, action_space = self.action_size, dueling = self.dueling)\n",
    "        self.target_model = OurModel(input_shape=self.state_size, action_space = self.action_size, dueling = self.dueling)  \n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self, game_steps):\n",
    "        if game_steps % self.update_model_steps == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            return\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        experience = state, action, reward, next_state, done\n",
    "        if self.USE_PER:\n",
    "            self.MEMORY.store(experience)\n",
    "        else:\n",
    "            self.memory.append((experience))\n",
    "\n",
    "    def act(self, state, decay_step):\n",
    "        # EPSILON GREEDY STRATEGY\n",
    "        if self.epsilon_greedy:\n",
    "        # Here we'll use an improved version of our epsilon greedy strategy for Q-learning\n",
    "            explore_probability = self.epsilon_min + (self.epsilon - self.epsilon_min) * np.exp(-self.epsilon_decay * decay_step)\n",
    "        # OLD EPSILON STRATEGY\n",
    "        else:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= (1-self.epsilon_decay)\n",
    "            explore_probability = self.epsilon\n",
    "    \n",
    "        if explore_probability > np.random.rand():\n",
    "            # Make a random action (exploration)\n",
    "            return random.randrange(self.action_size), explore_probability\n",
    "        else:\n",
    "            # Get action from Q-network (exploitation)\n",
    "            # Estimate the Qs values state\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            return np.argmax(self.model.predict(state)), explore_probability\n",
    "                \n",
    "    def replay(self):\n",
    "        if self.USE_PER:\n",
    "            # Sample minibatch from the PER memory\n",
    "            tree_idx, minibatch  = self.MEMORY.sample(self.batch_size)\n",
    "        else:\n",
    "            if len(self.memory) > self.batch_size:\n",
    "            # Randomly sample minibatch from the deque memory\n",
    "                minibatch = random.sample(self.memory, self.batch_size)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "        state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        action = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        reward = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        next_state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        done = np.zeros(self.batch_size, dtype=np.uint8)\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop       \n",
    "        for i in range(len(minibatch)):\n",
    "            state[i], action[i], reward[i], next_state[i], done[i] = minibatch[i]\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        # predict Q-values for starting state using the main network\n",
    "        target = self.model.predict(state)\n",
    "        target_old = np.array(target)\n",
    "        # predict best action in ending state using the main network\n",
    "        target_next = self.model.predict(next_state)\n",
    "        # predict Q-values for ending state using the target network\n",
    "        target_val = self.target_model.predict(next_state)\n",
    "\n",
    "\n",
    "        for i in range(len(minibatch)):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                if self.ddqn: # Double - DQN\n",
    "                    # current Q Network selects the action\n",
    "                    # a'_max = argmax_a' Q(s', a')\n",
    "                    a = np.argmax(target_next[i])\n",
    "                    # target Q Network evaluates the action\n",
    "                    # Q_max = Q_target(s', a'_max)\n",
    "                    target[i][action[i]] = reward[i] + self.gamma * target_val[i][a]\n",
    "                else: # Standard - DQN\n",
    "                    # DQN chooses the max Q value among next actions\n",
    "                    # selection and evaluation of action is on the target Q Network\n",
    "                    # Q_max = max_a' Q_target(s', a')\n",
    "                    # when using target model in simple DQN rules, we get better performance\n",
    "                    target[i][action[i]] = reward[i] + self.gamma * np.amax(target_val[i])\n",
    "            \n",
    "        if self.USE_PER:\n",
    "            indices = np.arange(self.batch_size, dtype=np.int32)\n",
    "            absolute_errors = np.abs(target_old[indices, action]-target[indices, action])\n",
    "\n",
    "            # Update priority\n",
    "            self.MEMORY.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model()\n",
    "\n",
    "    def save(self, name):\n",
    "        return\n",
    "        self.model.save(name)\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        pylab.plot(self.episodes, self.average, 'r')\n",
    "        pylab.plot(self.episodes, self.scores, 'b')\n",
    "        pylab.ylabel('Score', fontsize=18)\n",
    "        pylab.xlabel('Games', fontsize=18)\n",
    "        dqn = 'DQN_'\n",
    "        dueling = ''\n",
    "        greedy = ''\n",
    "        PER = ''\n",
    "        if self.ddqn: dqn = '_DDQN'\n",
    "        if self.dueling: dueling = '_Dueling'\n",
    "        if self.epsilon_greedy: greedy = '_Greedy'\n",
    "        if self.USE_PER: PER = '_PER'\n",
    "        try:\n",
    "            pylab.savefig(self.env_name+dqn+dueling+greedy+PER+\"_CNN.png\")\n",
    "        except OSError:\n",
    "            pass\n",
    "        # no need to worry about model, when doing a lot of experiments\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name+dqn+dueling+greedy+PER+\"_CNN.h5\")\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(\"cartpole\"+str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def GetImage(self, frame):\n",
    "        self.env.render()\n",
    "        \n",
    "        # croping frame to 80x80 size\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "            # OpenCV resize function \n",
    "            frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # converting to RGB (numpy way)\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "        # converting to RGB (OpenCV way)\n",
    "        #frame_rgb = cv2.cvtColor(frame_cropped, cv2.COLOR_RGB2GRAY)     \n",
    "\n",
    "        # dividing by 255 we expresses value to 0-1 representation\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        self.image_memory[0,:,:] = new_frame\n",
    "\n",
    "        # show image frame   \n",
    "        #self.imshow(self.image_memory,0)\n",
    "        #self.imshow(self.image_memory,1)\n",
    "        #self.imshow(self.image_memory,2)\n",
    "        #self.imshow(self.image_memory,3)\n",
    "        \n",
    "        return np.expand_dims(self.image_memory, axis=0)\n",
    "\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(frame)\n",
    "        return state\n",
    "\n",
    "    def step(self,action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = self.GetImage(next_state)\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def run(self):\n",
    "        decay_step = 0\n",
    "        max_average = -21.0\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            SAVING = ''\n",
    "            while not done:\n",
    "                decay_step += 1\n",
    "                action, explore_probability = self.act(state, decay_step)\n",
    "                next_state, reward, done, _ = self.step(action)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    # every episode, plot the result\n",
    "                    average = self.PlotModel(score, e)\n",
    "\n",
    "                    # saving best models\n",
    "                    if average >= max_average:\n",
    "                        max_average = average\n",
    "                        self.save(self.Model_name)\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2f}, average: {:.2f} {}\".format(e, self.EPISODES, score, explore_probability, average, SAVING))\n",
    "                    \n",
    "                # update target model\n",
    "                self.update_target_model(decay_step)\n",
    "\n",
    "                # train model\n",
    "                self.replay()\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self, Model_name):\n",
    "        self.load(Model_name)\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "        self.env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'BreakoutDeterministic-v4'\n",
    "    #env_name = 'Pong-v0'\n",
    "    agent = DQNAgent(env_name)\n",
    "    agent.run()\n",
    "    #agent.test('Models/Pong-v0_DQN_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lorenzo_env] *",
   "language": "python",
   "name": "conda-env-lorenzo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
