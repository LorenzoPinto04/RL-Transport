{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE ARRAY AS INPUT\n",
    "\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "from environment import City\n",
    "\n",
    "import pickle\n",
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense, Conv2D, Conv3D, Flatten, MaxPooling2D, MaxPooling3D\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import adam\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False\n",
    "exploration = True\n",
    "load_model = False\n",
    "\n",
    "\n",
    "def transform_reward(reward):\n",
    "    return np.sign(reward)\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "env = gym.make('CartPole-v0')\n",
    "# Reset it, returns the starting frame\n",
    "frame = env.reset()\n",
    "print('[INFO] Shape input:', frame.shape)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    \"\"\" Implementation of deep q learning algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, action_space, state_space, debug_mode = False, load_model = True):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1\n",
    "        # discount value \n",
    "        # 0 for present and 1 for future\n",
    "        self.gamma = .2\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        # epsilon denotes the fraction of time we will dedicate to exploring\n",
    "        #self.epsilon_min = .01\n",
    "        self.epsilon_min = .01\n",
    "        self.epsilon_decay = .995\n",
    "        self.learning_rate = 0.01\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        if load_model:\n",
    "            print(\"[INFO] Loading model from disk\")\n",
    "            # load json and create model\n",
    "            json_file = open('models/cartpole.json', 'r')\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "            self.model = model_from_json(loaded_model_json)\n",
    "            # load weights into new model\n",
    "            self.model.load_weights(\"models/cartpole.h5\")\n",
    "            self.model.compile(loss='mse', optimizer=adam(lr=self.learning_rate))\n",
    "            print(\"[INFO] Model loaded\")\n",
    "            return\n",
    "        #self.model = self.atari_model()\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    \n",
    "    def build_model1(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=(self.state_space,), activation='relu'))\n",
    "        #model.add(Dense(64, input_dim=2, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_model(self):\n",
    "        input_shape= (self.state_space,)\n",
    "        action_space= (self.action_space)\n",
    "        X_input = Input(input_shape)\n",
    "\n",
    "        # 'Dense' is the basic form of a neural network layer\n",
    "        # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "        X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "\n",
    "        # Hidden layer with 256 nodes\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        # Hidden layer with 64 nodes\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        # Output Layer with # of actions: 2 nodes (left, right)\n",
    "        X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "        model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if debug_mode:    \n",
    "            print('State: \\n', state[0].reshape(y_size, x_size))\n",
    "            print('action: ', action)\n",
    "            print('Next state: \\n', next_state[0].reshape(y_size, x_size))\n",
    "            print('reward: ', reward)\n",
    "            print('------------------------------------------------')\n",
    "\n",
    "    def act(self, state):\n",
    "        # if the random float is smaller than epsilon reduced, it takes a random action (explore)\n",
    "        if np.random.rand() <= self.epsilon and exploration:\n",
    "            #print('Exploration step')\n",
    "            return random.randrange(self.action_space)\n",
    "        # else exploit\n",
    "        #state = state.reshape((1, 2, 105, 80, 1))\n",
    "        state = state.reshape((1,self.state_space))\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "        \n",
    "        '''\n",
    "        print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "        print('States -------------------------------------------------------------------------')\n",
    "        plt.imshow(states[0][0].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        plt.imshow(states[0][1].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        \n",
    "        print('Next states -------------------------------------------------------------------------')\n",
    "        plt.imshow(next_states[0][0].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        plt.imshow(next_states[0][1].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # First, predict the Q values of the next states. Note how we are passing ones as the mask.\n",
    "        next_Q_values = self.model.predict(next_states)\n",
    "        # The Q values of the terminal states is 0 by definition, so override them\n",
    "        next_Q_values[dones] = 0\n",
    "        # The Q values of each start state is the reward + gamma * the max next state Q value\n",
    "        targets = rewards + self.gamma * np.max(next_Q_values, axis=1)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        \n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        # every new iteration reduce epsilon to push the exploration\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return self.model\n",
    "    \n",
    "    \n",
    "\n",
    "def train_dqn(episode):\n",
    "    print(episode)\n",
    "    loss = []\n",
    "    agent = DQN(2, 4, debug_mode, load_model)\n",
    "    for e in range(episode):\n",
    "        state = env.reset()\n",
    "        #set_states = np.array([state, state])\n",
    "        score = 0\n",
    "        max_steps = 10000\n",
    "        \n",
    "        #set_states_past = np.array([state, state])\n",
    "        #set_states_fut = np.array([state])\n",
    "        \n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            \n",
    "            #print(state.shape)\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            #action = agent.act(set_states)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            #set_states_fut = np.array([set_states_fut[-1], next_state])\n",
    "            \n",
    "            score += reward\n",
    "            \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            #agent.remember(set_states_past, action, reward, set_states_fut, done)\n",
    "            state = next_state\n",
    "\n",
    "            #set_states_past = np.array([set_states_past[-1], next_state])\n",
    "\n",
    "            \n",
    "            model = agent.replay()\n",
    "            env.render()\n",
    "            if done:\n",
    "                break\n",
    "        loss.append(score)\n",
    "        print(\"episode: {}/{}, moves:{}, score: {}\".format(e, episode, i, str(score)[:4]))\n",
    "        if (e+1) % 100 == 0:\n",
    "            '''\n",
    "            print('[INFO] Saving checkpoint iter:', e)\n",
    "            # serialize model to JSON\n",
    "            model_json = model.to_json()\n",
    "            with open(\"models/cartpole.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(\"models/cartpole.h5\")\n",
    "            print(\"[INFO] Saved model to disk\")\n",
    "            # with open(r\"models/model_auto6.pickle\", \"wb\") as f:\n",
    "            #    pickle.dump(agent, f)\n",
    "            '''\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot([i for i in range(e)], loss[-e:])\n",
    "            plt.xlabel('episodes')\n",
    "            plt.ylabel('reward')\n",
    "            #plt.savefig('training_graph_check{}'.format(e))\n",
    "            plt.show()\n",
    "    return loss\n",
    "\n",
    "ep = 100000\n",
    "loss = train_dqn(ep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "\n",
    "    # Hidden layer with 256 nodes\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    \n",
    "    # Hidden layer with 64 nodes\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        #self.env = gym.make('MountainCar-v0')\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        # by default, CartPole-v1 has max episode steps = 500\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        \n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create main model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.train_start:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # Standard - DQN\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                # selection and evaluation of action is on the target Q Network\n",
    "                # Q_max = max_a' Q_target(s', a')\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "            \n",
    "    def run(self):\n",
    "        self.load(\"cartpole-dqn.h5\")\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                if not done or i == self.env._max_episode_steps-1:\n",
    "                    reward = reward\n",
    "                else:\n",
    "                    reward = -100\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                score += reward\n",
    "                if done:                   \n",
    "                    print(\"episode: {}/{}, steps: {}, score: {}, e: {:.2}\".format(e, self.EPISODES, i, score, self.epsilon))\n",
    "                    if e == 1000:\n",
    "                        print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                        self.save(\"cartpole-dqn.h5\")\n",
    "                        return\n",
    "                self.replay()\n",
    "\n",
    "    def test(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
    "                    break\n",
    "\n",
    "                    \n",
    "agent = DQNAgent()\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SumTree(object):\n",
    "    data_pointer = 0\n",
    "    \n",
    "    # Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    def __init__(self, capacity):\n",
    "        # Number of leaf nodes (final nodes) that contains experiences\n",
    "        self.capacity = capacity\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2 * capacity - 1) look at the schema below\n",
    "        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        \n",
    "        # Contains the experiences (so the size of data is capacity)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "    \n",
    "    \n",
    "    # Here we define function that will add our priority score in the sumtree leaf and add the experience in data:\n",
    "    def add(self, priority, data):\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "\n",
    "        # Update data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "\n",
    "        # Update the leaf\n",
    "        self.update (tree_index, priority)\n",
    "\n",
    "        # Add 1 to data_pointer\n",
    "        self.data_pointer += 1\n",
    "\n",
    "        if self.data_pointer >= self.capacity:  # If we're above the capacity, we go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    # Update the leaf priority score and propagate the change through tree\n",
    "    def update(self, tree_index, priority):\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "\n",
    "        # then propagate the change through tree\n",
    "        # this method is faster than the recursive loop in the reference code\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "        \n",
    "    # Here build a function to get a leaf from our tree. So we'll build a function to get the leaf_index, priority value of that leaf and experience associated with that leaf index:\n",
    "    def get_leaf(self, v):\n",
    "        parent_index = 0\n",
    "\n",
    "        # the while loop is faster than the method in the reference code\n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "\n",
    "            # If we reach bottom, end the search\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else: # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[left_child_index]:\n",
    "                    parent_index = left_child_index\n",
    "                else:\n",
    "                    v -= self.tree[left_child_index]\n",
    "                    parent_index = right_child_index\n",
    "\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        return self.tree[0] # Returns the root node\n",
    "\n",
    "# Now we finished constructing our SumTree object, next we'll build a memory object.\n",
    "class Memory(object):  # stored as ( state, action, reward, next_state ) in SumTree\n",
    "    PER_e = 0.01  # Hyperparameter that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    PER_b = 0.4  # importance-sampling, from initial value increasing to 1\n",
    "    \n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    \n",
    "    absolute_error_upper = 1.  # clipped abs error\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        # Making the tree \n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    # Next, we define a function to store a new experience in our tree.\n",
    "    # Each new experience will have a score of max_prority (it will be then improved when we use this exp to train our DDQN).\n",
    "    def store(self, experience):\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "\n",
    "        # If the max priority = 0 we can't put priority = 0 since this experience will never have a chance to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0:\n",
    "            max_priority = self.absolute_error_upper\n",
    "\n",
    "        self.tree.add(max_priority, experience)   # set the max priority for new priority\n",
    "        \n",
    "    # Now we create sample function, which will be used to pick batch from our tree memory, which will be used to train our model.\n",
    "    # - First, we sample a minibatch of n size, the range [0, priority_total] into priority ranges.\n",
    "    # - Then a value is uniformly sampled from each range.\n",
    "    # - Then we search in the sumtree, for the experience where priority score correspond to sample values are retrieved from.\n",
    "    def sample(self, n):\n",
    "        # Create a minibatch array that will contains the minibatch\n",
    "        minibatch = []\n",
    "\n",
    "        b_idx = np.empty((n,), dtype=np.int32)\n",
    "\n",
    "        # Calculate the priority segment\n",
    "        # Here, as explained in the paper, we divide the Range[0, ptotal] into n ranges\n",
    "        priority_segment = self.tree.total_priority / n       # priority segment\n",
    "\n",
    "        for i in range(n):\n",
    "            # A value is uniformly sample from each range\n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a, b)\n",
    "\n",
    "            # Experience that correspond to each value is retrieved\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "\n",
    "            b_idx[i]= index\n",
    "\n",
    "            minibatch.append([data[0],data[1],data[2],data[3],data[4]])\n",
    "\n",
    "        return b_idx, minibatch\n",
    "    \n",
    "    # Update the priorities on the tree\n",
    "    def batch_update(self, tree_idx, abs_errors):\n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors, self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors, self.PER_a)\n",
    "\n",
    "        for ti, p in zip(tree_idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__ is 2.1.0\n",
      "tf.keras.__version__ is: 2.2.4-tf\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "\n",
    "print(\"tf.__version__ is\", tf.__version__)\n",
    "print(\"tf.keras.__version__ is:\", tf.keras.__version__)\n",
    "\n",
    "def _get_available_gpus():\n",
    "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
    "\n",
    "    # Returns\n",
    "        A list of available GPU devices.\n",
    "    \"\"\"\n",
    "    #global _LOCAL_DEVICES\n",
    "    if tfback._LOCAL_DEVICES is None:\n",
    "        devices = tf.config.list_logical_devices()\n",
    "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "\n",
    "tfback._get_available_gpus = _get_available_gpus\n",
    "\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 4, 80, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 19, 19)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 8, 8)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, 6, 6)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,406,310\n",
      "Trainable params: 1,406,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 4, 80, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 19, 19)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 64, 8, 8)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 64, 6, 6)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,406,310\n",
      "Trainable params: 1,406,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0/1000, score: 5.0\n",
      "episode: 1/1000, score: -1.0\n",
      "episode: 2/1000, score: 4.0\n",
      "episode: 3/1000, score: 3.0\n",
      "episode: 4/1000, score: 1.0\n",
      "episode: 5/1000, score: -3.0\n",
      "episode: 6/1000, score: 10.0\n",
      "episode: 7/1000, score: 8.0\n",
      "episode: 8/1000, score: 16.0\n",
      "episode: 9/1000, score: 7.0\n",
      "episode: 10/1000, score: 3.0\n",
      "episode: 11/1000, score: -1.0\n",
      "episode: 12/1000, score: 11.0\n",
      "episode: 13/1000, score: 5.0\n",
      "episode: 14/1000, score: 2.0\n",
      "episode: 15/1000, score: -7.0\n",
      "episode: 16/1000, score: 6.0\n",
      "episode: 17/1000, score: 6.0\n",
      "episode: 18/1000, score: -2.0\n",
      "episode: 19/1000, score: 7.0\n",
      "episode: 20/1000, score: 6.0\n",
      "episode: 21/1000, score: -4.0\n",
      "episode: 22/1000, score: 5.0\n",
      "episode: 23/1000, score: -6.0\n",
      "episode: 24/1000, score: 7.0\n",
      "episode: 25/1000, score: -7.0\n",
      "episode: 26/1000, score: -2.0\n",
      "episode: 27/1000, score: 2.0\n",
      "episode: 28/1000, score: 6.0\n",
      "episode: 29/1000, score: 9.0\n",
      "episode: 30/1000, score: 6.0\n",
      "episode: 31/1000, score: 3.0\n",
      "episode: 32/1000, score: 6.0\n",
      "episode: 33/1000, score: 5.0\n",
      "episode: 34/1000, score: 10.0\n",
      "episode: 35/1000, score: -6.0\n",
      "episode: 36/1000, score: 8.0\n",
      "episode: 37/1000, score: -9.0\n",
      "episode: 38/1000, score: -1.0\n",
      "episode: 39/1000, score: 6.0\n",
      "episode: 40/1000, score: 6.0\n",
      "episode: 41/1000, score: 13.0\n",
      "episode: 42/1000, score: 12.0\n",
      "episode: 43/1000, score: 10.0\n",
      "episode: 44/1000, score: 9.0\n",
      "episode: 45/1000, score: -4.0\n",
      "episode: 46/1000, score: -3.0\n",
      "episode: 47/1000, score: 10.0\n",
      "episode: 48/1000, score: 9.0\n",
      "episode: 49/1000, score: 5.0\n",
      "episode: 50/1000, score: -5.0\n",
      "episode: 51/1000, score: 5.0\n",
      "episode: 52/1000, score: 3.0\n",
      "episode: 53/1000, score: 10.0\n",
      "episode: 54/1000, score: -13.0\n",
      "episode: 55/1000, score: -10.0\n",
      "episode: 56/1000, score: 2.0\n",
      "episode: 57/1000, score: 15.0\n",
      "episode: 58/1000, score: 3.0\n",
      "episode: 59/1000, score: -8.0\n",
      "episode: 60/1000, score: 14.0\n",
      "episode: 61/1000, score: 2.0\n",
      "episode: 62/1000, score: 2.0\n",
      "episode: 63/1000, score: 7.0\n",
      "episode: 64/1000, score: 11.0\n",
      "episode: 65/1000, score: 7.0\n",
      "episode: 66/1000, score: 10.0\n",
      "episode: 67/1000, score: 9.0\n",
      "episode: 68/1000, score: 11.0\n",
      "episode: 69/1000, score: 1.0\n",
      "episode: 70/1000, score: 1.0\n",
      "episode: 71/1000, score: 9.0\n",
      "episode: 72/1000, score: -2.0\n",
      "episode: 73/1000, score: 10.0\n",
      "episode: 74/1000, score: 1.0\n",
      "episode: 75/1000, score: 7.0\n",
      "episode: 76/1000, score: -7.0\n",
      "episode: 77/1000, score: -3.0\n",
      "episode: 78/1000, score: 11.0\n",
      "episode: 79/1000, score: 10.0\n",
      "episode: 80/1000, score: 4.0\n",
      "episode: 81/1000, score: 10.0\n",
      "episode: 82/1000, score: 1.0\n",
      "episode: 83/1000, score: 5.0\n",
      "episode: 84/1000, score: 10.0\n",
      "episode: 85/1000, score: 7.0\n",
      "episode: 86/1000, score: 5.0\n",
      "episode: 87/1000, score: 7.0\n",
      "episode: 88/1000, score: 2.0\n",
      "episode: 89/1000, score: 9.0\n"
     ]
    }
   ],
   "source": [
    "# Tutorial by www.pylessons.com\n",
    "# Tutorial written for - Tensorflow 1.15, Keras 2.2.4\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def OurModel(input_shape, action_space, dueling):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    #X = Conv2D(64, 5, strides=(3, 3),padding=\"valid\", input_shape=input_shape, activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(32, 8, strides=(4, 4),padding=\"valid\", input_shape=input_shape, activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(64, 4, strides=(2, 2),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(64, 3, strides=(1, 1),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Flatten()(X)\n",
    "    \n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    X = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    if dueling:\n",
    "        state_value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "        state_value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(action_space,))(state_value)\n",
    "\n",
    "        action_advantage = Dense(action_space, kernel_initializer='he_uniform')(X)\n",
    "        action_advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(action_space,))(action_advantage)\n",
    "\n",
    "        X = Add()([state_value, action_advantage])\n",
    "    else:\n",
    "        # Output Layer with # of actions: 2 nodes (left, right)\n",
    "        X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    #model.compile(loss=\"mean_squared_error\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "    #model.compile(optimizer=Adam(lr=0.00025), loss='mean_squared_error')\n",
    "    model.compile(optimizer=Adam(lr=0.00005), loss='mean_squared_error')\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env_name):\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        #self.env.seed(0)  \n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        \n",
    "        # Instantiate memory\n",
    "        memory_size = 25000\n",
    "        self.MEMORY = Memory(memory_size)\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        \n",
    "        # EXPLORATION HYPERPARAMETERS for epsilon and epsilon greedy strategy\n",
    "        self.epsilon = 1.0  # exploration probability at start\n",
    "        self.epsilon_min = 0.02  # minimum exploration probability \n",
    "        self.epsilon_decay = 0.00002  # exponential decay rate for exploration prob\n",
    "        \n",
    "        self.batch_size = 32\n",
    "\n",
    "        # defining model parameters\n",
    "        self.ddqn = False # use doudle deep q network\n",
    "        self.dueling = False # use dealing netowrk\n",
    "        self.epsilon_greedy = False # use epsilon greedy strategy\n",
    "        self.USE_PER = False # use priority experienced replay\n",
    "        \n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name+\"_CNN.h5\")\n",
    "\n",
    "        self.ROWS = 80\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 4\n",
    "        self.update_model_steps = 1000\n",
    "        \n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "        \n",
    "        # create main model and target model\n",
    "        self.model = OurModel(input_shape=self.state_size, action_space = self.action_size, dueling = self.dueling)\n",
    "        self.target_model = OurModel(input_shape=self.state_size, action_space = self.action_size, dueling = self.dueling)  \n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self, game_steps):\n",
    "        if game_steps % self.update_model_steps == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            return\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        experience = state, action, reward, next_state, done\n",
    "        if self.USE_PER:\n",
    "            self.MEMORY.store(experience)\n",
    "        else:\n",
    "            self.memory.append((experience))\n",
    "\n",
    "    def act(self, state, decay_step):\n",
    "        # EPSILON GREEDY STRATEGY\n",
    "        if self.epsilon_greedy:\n",
    "        # Here we'll use an improved version of our epsilon greedy strategy for Q-learning\n",
    "            explore_probability = self.epsilon_min + (self.epsilon - self.epsilon_min) * np.exp(-self.epsilon_decay * decay_step)\n",
    "        # OLD EPSILON STRATEGY\n",
    "        else:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= (1-self.epsilon_decay)\n",
    "            explore_probability = self.epsilon\n",
    "    \n",
    "        if explore_probability > np.random.rand():\n",
    "            # Make a random action (exploration)\n",
    "            return random.randrange(self.action_size), explore_probability\n",
    "        else:\n",
    "            # Get action from Q-network (exploitation)\n",
    "            # Estimate the Qs values state\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            return np.argmax(self.model.predict(state)), explore_probability\n",
    "                \n",
    "    def replay(self):\n",
    "        if self.USE_PER:\n",
    "            # Sample minibatch from the PER memory\n",
    "            tree_idx, minibatch  = self.MEMORY.sample(self.batch_size)\n",
    "        else:\n",
    "            if len(self.memory) > self.batch_size:\n",
    "            # Randomly sample minibatch from the deque memory\n",
    "                minibatch = random.sample(self.memory, self.batch_size)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "        state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        action = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        reward = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        next_state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        done = np.zeros(self.batch_size, dtype=np.uint8)\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop       \n",
    "        for i in range(len(minibatch)):\n",
    "            state[i], action[i], reward[i], next_state[i], done[i] = minibatch[i]\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        # predict Q-values for starting state using the main network\n",
    "        target = self.model.predict(state)\n",
    "        target_old = np.array(target)\n",
    "        # predict best action in ending state using the main network\n",
    "        target_next = self.model.predict(next_state)\n",
    "        # predict Q-values for ending state using the target network\n",
    "        target_val = self.target_model.predict(next_state)\n",
    "\n",
    "\n",
    "        for i in range(len(minibatch)):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                if self.ddqn: # Double - DQN\n",
    "                    # current Q Network selects the action\n",
    "                    # a'_max = argmax_a' Q(s', a')\n",
    "                    a = np.argmax(target_next[i])\n",
    "                    # target Q Network evaluates the action\n",
    "                    # Q_max = Q_target(s', a'_max)\n",
    "                    target[i][action[i]] = reward[i] + self.gamma * target_val[i][a]\n",
    "                else: # Standard - DQN\n",
    "                    # DQN chooses the max Q value among next actions\n",
    "                    # selection and evaluation of action is on the target Q Network\n",
    "                    # Q_max = max_a' Q_target(s', a')\n",
    "                    # when using target model in simple DQN rules, we get better performance\n",
    "                    target[i][action[i]] = reward[i] + self.gamma * np.amax(target_val[i])\n",
    "            \n",
    "        if self.USE_PER:\n",
    "            indices = np.arange(self.batch_size, dtype=np.int32)\n",
    "            absolute_errors = np.abs(target_old[indices, action]-target[indices, action])\n",
    "\n",
    "            # Update priority\n",
    "            self.MEMORY.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        return\n",
    "        self.model.save(name)\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        pylab.plot(self.episodes, self.average, 'r')\n",
    "        pylab.plot(self.episodes, self.scores, 'b')\n",
    "        pylab.ylabel('Score', fontsize=18)\n",
    "        pylab.xlabel('Games', fontsize=18)\n",
    "        dqn = 'DQN_'\n",
    "        dueling = ''\n",
    "        greedy = ''\n",
    "        PER = ''\n",
    "        if self.ddqn: dqn = '_DDQN'\n",
    "        if self.dueling: dueling = '_Dueling'\n",
    "        if self.epsilon_greedy: greedy = '_Greedy'\n",
    "        if self.USE_PER: PER = '_PER'\n",
    "        try:\n",
    "            pylab.savefig(self.env_name+dqn+dueling+greedy+PER+\"_CNN.png\")\n",
    "        except OSError:\n",
    "            pass\n",
    "        # no need to worry about model, when doing a lot of experiments\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name+dqn+dueling+greedy+PER+\"_CNN.h5\")\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(\"cartpole\"+str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def GetImage(self, frame):\n",
    "        self.env.render()\n",
    "        \n",
    "        # croping frame to 80x80 size\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "            # OpenCV resize function \n",
    "            frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # converting to RGB (numpy way)\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "        # converting to RGB (OpenCV way)\n",
    "        #frame_rgb = cv2.cvtColor(frame_cropped, cv2.COLOR_RGB2GRAY)     \n",
    "\n",
    "        # dividing by 255 we expresses value to 0-1 representation\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        self.image_memory[0,:,:] = new_frame\n",
    "\n",
    "        # show image frame   \n",
    "        #self.imshow(self.image_memory,0)\n",
    "        #self.imshow(self.image_memory,1)\n",
    "        #self.imshow(self.image_memory,2)\n",
    "        #self.imshow(self.image_memory,3)\n",
    "        \n",
    "        return np.expand_dims(self.image_memory, axis=0)\n",
    "\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(frame)\n",
    "        return state\n",
    "\n",
    "    def step(self,action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = self.GetImage(next_state)\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def run(self):\n",
    "        decay_step = 0\n",
    "        max_average = -21.0\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            SAVING = ''\n",
    "            while not done:\n",
    "                decay_step += 1\n",
    "                action, explore_probability = self.act(state, decay_step)\n",
    "                next_state, reward, done, _ = self.step(action)\n",
    "                '''\n",
    "                if reward != 0.0:\n",
    "                    print('States -------------------------------------------------------------------------')\n",
    "                    plt.imshow(state[0][0])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][1])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][2])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][3])\n",
    "                    plt.show()\n",
    "                    print('Action:', action)\n",
    "                    print('Reward:', reward)\n",
    "                    '''\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    # every episode, plot the result\n",
    "                    average = self.PlotModel(score, e)\n",
    "\n",
    "                    # saving best models\n",
    "                    if average >= max_average:\n",
    "                        max_average = average\n",
    "                        self.save(self.Model_name)\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2f}, average: {:.2f} {}\".format(e, self.EPISODES, score, explore_probability, average, SAVING))\n",
    "                    \n",
    "                # update target model\n",
    "                self.update_target_model(decay_step)\n",
    "\n",
    "                # train model\n",
    "                self.replay()\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self, Model_name):\n",
    "        import matplotlib.pyplot as plt    \n",
    "        import time\n",
    "        self.load(Model_name)\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                time.sleep(.01)\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = self.GetImage(next_state)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "        self.env.close()\n",
    "\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    #env_name = 'BreakoutDeterministic-v4'\n",
    "    env_name = 'Pong-v0'\n",
    "    agent = DQNAgent(env_name)\n",
    "    #agent.run()\n",
    "    agent.test('Models/Pong-v0DQN__CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lorenzo_env] *",
   "language": "python",
   "name": "conda-env-lorenzo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
