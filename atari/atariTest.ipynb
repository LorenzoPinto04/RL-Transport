{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE ARRAY AS INPUT\n",
    "\n",
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "from environment import City\n",
    "\n",
    "import pickle\n",
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense, Conv2D, Conv3D, Flatten, MaxPooling2D, MaxPooling3D\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import adam\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "\n",
    "debug_mode = False\n",
    "exploration = True\n",
    "load_model = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    img = to_grayscale(downsample(img))\n",
    "    img = img.reshape((105, 80, 1))\n",
    "    return img\n",
    "\n",
    "\n",
    "def transform_reward(reward):\n",
    "    return np.sign(reward)\n",
    "\n",
    "# Import the gym module\n",
    "import gym\n",
    "\n",
    "# Create a breakout environment\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "# Reset it, returns the starting frame\n",
    "frame = env.reset()\n",
    "print('[INFO] Shape input:', preprocess(frame).shape)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQN:\n",
    "\n",
    "    \"\"\" Implementation of deep q learning algorithm \"\"\"\n",
    "\n",
    "    def __init__(self, action_space, state_space, debug_mode = False, load_model = True):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.epsilon = 1\n",
    "        # discount value \n",
    "        # 0 for present and 1 for future\n",
    "        self.gamma = .2\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        # epsilon denotes the fraction of time we will dedicate to exploring\n",
    "        #self.epsilon_min = .01\n",
    "        self.epsilon_min = .01\n",
    "        self.epsilon_decay = .995\n",
    "        self.learning_rate = 0.01\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        if load_model:\n",
    "            print(\"[INFO] Loading model from disk\")\n",
    "            # load json and create model\n",
    "            json_file = open('models/CNN_atari.json', 'r')\n",
    "            loaded_model_json = json_file.read()\n",
    "            json_file.close()\n",
    "            self.model = model_from_json(loaded_model_json)\n",
    "            # load weights into new model\n",
    "            self.model.load_weights(\"models/CNN_atari.h5\")\n",
    "            self.model.compile(loss='mse', optimizer=adam(lr=self.learning_rate))\n",
    "            print(\"[INFO] Model loaded\")\n",
    "            return\n",
    "        #self.model = self.atari_model()\n",
    "        self.model = self.build_model_conv()\n",
    "\n",
    "    \n",
    "    def build_model_conv(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv3D(16, (2, 8, 8),  activation='relu', \n",
    "                         input_shape=(2, self.state_space[0], self.state_space[1], 1)))\n",
    "        #model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "        model.add(Conv3D(32, (1, 4, 4), activation='relu'))\n",
    "        #model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "        #model.add(Conv2D(64, (1, 1), activation='relu', \n",
    "        #                 input_shape=(self.state_space[0], self.state_space[1], 3)))\n",
    "        #model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation=\"relu\"))\n",
    "        model.add(Dense(4))\n",
    "        model.compile(loss=\"mse\",\n",
    "                           optimizer=adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if debug_mode:    \n",
    "            print('State: \\n', state[0].reshape(y_size, x_size))\n",
    "            print('action: ', action)\n",
    "            print('Next state: \\n', next_state[0].reshape(y_size, x_size))\n",
    "            print('reward: ', reward)\n",
    "            print('------------------------------------------------')\n",
    "\n",
    "    def act(self, state):\n",
    "        # if the random float is smaller than epsilon reduced, it takes a random action (explore)\n",
    "        if np.random.rand() <= self.epsilon and exploration:\n",
    "            #print('Exploration step')\n",
    "            return random.randrange(self.action_space)\n",
    "        # else exploit\n",
    "        state = state.reshape((1, 2, 105, 80, 1))\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "        \n",
    "        '''\n",
    "        print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "        print('States -------------------------------------------------------------------------')\n",
    "        plt.imshow(states[0][0].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        plt.imshow(states[0][1].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        \n",
    "        print('Next states -------------------------------------------------------------------------')\n",
    "        plt.imshow(next_states[0][0].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        plt.imshow(next_states[0][1].squeeze(axis=2))\n",
    "        plt.show()\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # First, predict the Q values of the next states. Note how we are passing ones as the mask.\n",
    "        next_Q_values = self.model.predict(next_states)\n",
    "        # The Q values of the terminal states is 0 by definition, so override them\n",
    "        next_Q_values[dones] = 0\n",
    "        # The Q values of each start state is the reward + gamma * the max next state Q value\n",
    "        targets = rewards + self.gamma * np.max(next_Q_values, axis=1)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        \n",
    "        ind = np.array([i for i in range(self.batch_size)])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        # every new iteration reduce epsilon to push the exploration\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return self.model\n",
    "    \n",
    "    \n",
    "\n",
    "def train_dqn(episode):\n",
    "    print(episode)\n",
    "    loss = []\n",
    "    agent = DQN(4, (105, 80), debug_mode, load_model)\n",
    "    for e in range(episode):\n",
    "        state = (preprocess(env.reset()))\n",
    "        set_states = np.array([state, state])\n",
    "        score = 0\n",
    "        max_steps = 10000\n",
    "        \n",
    "        set_states_past = np.array([state, state])\n",
    "        set_states_fut = np.array([state])\n",
    "        \n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            action = agent.act(set_states)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = preprocess(next_state)\n",
    "            \n",
    "            set_states_fut = np.array([set_states_fut[-1], next_state])\n",
    "            \n",
    "            score += reward\n",
    "            \n",
    "            agent.remember(set_states_past, action, reward, set_states_fut, done)\n",
    "            state = next_state\n",
    "\n",
    "            set_states_past = np.array([set_states_past[-1], next_state])\n",
    "\n",
    "            \n",
    "            model = agent.replay()\n",
    "            env.render()\n",
    "            if done:\n",
    "                break\n",
    "        loss.append(score)\n",
    "        print(\"episode: {}/{}, moves:{}, score: {}\".format(e, episode, i, str(score)[:4]))\n",
    "        if (e+1) % 1000 == 0:\n",
    "            print('[INFO] Saving checkpoint iter:', e)\n",
    "            # serialize model to JSON\n",
    "            model_json = model.to_json()\n",
    "            with open(\"models/CNN_atari.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(\"models/CNN_atari.h5\")\n",
    "            print(\"[INFO] Saved model to disk\")\n",
    "            # with open(r\"models/model_auto6.pickle\", \"wb\") as f:\n",
    "            #    pickle.dump(agent, f)\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.plot([i for i in range(e)], loss[-e:])\n",
    "            plt.xlabel('episodes')\n",
    "            plt.ylabel('reward')\n",
    "            #plt.savefig('training_graph_check{}'.format(e))\n",
    "            plt.show()\n",
    "    return loss\n",
    "\n",
    "ep = 100000\n",
    "loss = train_dqn(ep)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    img = to_grayscale(downsample(img))\n",
    "    img = img.reshape((105, 80, 1))\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def OurModel(input_shape, action_space):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n",
    "    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "\n",
    "    # Hidden layer with 256 nodes\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "    \n",
    "    # Hidden layer with 64 nodes\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X, name='CartPole DQN model')\n",
    "    model.compile(loss=\"mse\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('BreakoutDeterministic-v4')\n",
    "        # by default, CartPole-v1 has max episode steps = 500\n",
    "        #self.state_size = self.env.observation_space.shape[0]\n",
    "        self.state_size = 8400\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES = 1000\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        \n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        #self.epsilon_min = 0.001\n",
    "        self.epsilon_min = 0.2\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create main model\n",
    "        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if len(self.memory) > self.train_start:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        # Randomly sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "\n",
    "        state = np.zeros((self.batch_size, self.state_size))\n",
    "        next_state = np.zeros((self.batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop\n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = minibatch[i][0]\n",
    "            action.append(minibatch[i][1])\n",
    "            reward.append(minibatch[i][2])\n",
    "            next_state[i] = minibatch[i][3]\n",
    "            done.append(minibatch[i][4])\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # Standard - DQN\n",
    "                # DQN chooses the max Q value among next actions\n",
    "                # selection and evaluation of action is on the target Q Network\n",
    "                # Q_max = max_a' Q_target(s', a')\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "            \n",
    "    def run(self):\n",
    "        score = 0\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = preprocess(state)\n",
    "            state = state.reshape(1, self.state_size)\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = preprocess(next_state)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                score += reward\n",
    "                if done:   \n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.EPISODES, score, self.epsilon))\n",
    "                    score = 0\n",
    "                    if e == 1000:\n",
    "                        print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                        self.save(\"cartpole-dqn.h5\")\n",
    "                        return\n",
    "                self.replay()\n",
    "\n",
    "    def test(self):\n",
    "        self.load(\"cartpole-dqn.h5\")\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(next_state, [1, self.state_size])\n",
    "                i += 1\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
    "                    break\n",
    "\n",
    "                    \n",
    "agent = DQNAgent()\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
