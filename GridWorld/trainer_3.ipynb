{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lorenzo/anaconda3/envs/lorenzo_env/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Model created\n",
      "Run: 1, exploration: 1.0, score: -64. step: 100\n",
      "Run: 2, exploration: 1.0, score: -68. step: 100\n",
      "Run: 3, exploration: 0.99, score: -13. step: 20\n",
      "Run: 4, exploration: 0.98, score: -7.8 step: 20\n",
      "Run: 5, exploration: 0.97, score: -14. step: 20\n",
      "Run: 6, exploration: 0.96, score: -6.2 step: 20\n",
      "Run: 7, exploration: 0.95, score: -15. step: 20\n",
      "Run: 8, exploration: 0.94, score: -11. step: 20\n",
      "Run: 9, exploration: 0.93, score: -6.1 step: 20\n",
      "Run: 10, exploration: 0.92, score: -16. step: 20\n",
      "Run: 11, exploration: 0.91, score: -4.0 step: 20\n",
      "Run: 12, exploration: 0.90, score: -10. step: 20\n",
      "Run: 13, exploration: 0.90, score: -7.5 step: 20\n",
      "Run: 14, exploration: 0.89, score: -10. step: 20\n",
      "Run: 15, exploration: 0.88, score: 28.7 step: 12\n",
      "Run: 16, exploration: 0.87, score: -11. step: 20\n",
      "Run: 17, exploration: 0.87, score: 25.4 step: 19\n",
      "Run: 18, exploration: 0.86, score: -9.9 step: 20\n",
      "Run: 19, exploration: 0.85, score: -9.8 step: 20\n",
      "Run: 20, exploration: 0.84, score: -7.6 step: 20\n",
      "model saved\n",
      "Run: 21, exploration: 0.84, score: 37.1 step: 8\n",
      "Run: 22, exploration: 0.83, score: 29.6 step: 15\n",
      "Run: 23, exploration: 0.82, score: -2.8 step: 20\n",
      "Run: 24, exploration: 0.82, score: -6.7 step: 20\n",
      "Run: 25, exploration: 0.81, score: -8.8 step: 20\n",
      "Run: 26, exploration: 0.80, score: -14. step: 20\n",
      "Run: 27, exploration: 0.79, score: -10. step: 20\n",
      "Run: 28, exploration: 0.79, score: 33.8 step: 10\n",
      "Run: 29, exploration: 0.78, score: -6.9 step: 20\n",
      "Run: 30, exploration: 0.78, score: -7.1 step: 20\n",
      "Run: 31, exploration: 0.77, score: -16. step: 20\n",
      "Run: 32, exploration: 0.76, score: -6.2 step: 20\n",
      "Run: 33, exploration: 0.75, score: -11. step: 20\n",
      "Run: 34, exploration: 0.75, score: -7.6 step: 20\n",
      "Run: 35, exploration: 0.74, score: 33.7 step: 8\n",
      "Run: 36, exploration: 0.74, score: -9.2 step: 20\n",
      "Run: 37, exploration: 0.73, score: -5.1 step: 20\n",
      "Run: 38, exploration: 0.72, score: -3.7 step: 20\n",
      "Run: 39, exploration: 0.72, score: -6.8 step: 20\n",
      "Run: 40, exploration: 0.71, score: -9.9 step: 20\n",
      "model saved\n",
      "Run: 41, exploration: 0.70, score: -10. step: 20\n",
      "Run: 42, exploration: 0.70, score: -14. step: 20\n",
      "Run: 43, exploration: 0.69, score: -4.4 step: 20\n",
      "Run: 44, exploration: 0.68, score: 26.9 step: 14\n",
      "Run: 45, exploration: 0.68, score: -11. step: 20\n",
      "Run: 46, exploration: 0.67, score: -10. step: 20\n",
      "Run: 47, exploration: 0.66, score: -11. step: 20\n",
      "Run: 48, exploration: 0.66, score: 33.8 step: 10\n",
      "Run: 49, exploration: 0.66, score: -10. step: 20\n",
      "Run: 50, exploration: 0.65, score: 37.1 step: 8\n",
      "Run: 51, exploration: 0.65, score: -14. step: 20\n",
      "Run: 52, exploration: 0.65, score: 37.9 step: 7\n",
      "Run: 53, exploration: 0.64, score: -10. step: 20\n",
      "Run: 54, exploration: 0.63, score: 31.4 step: 15\n",
      "Run: 55, exploration: 0.63, score: 31.0 step: 11\n",
      "Run: 56, exploration: 0.63, score: -9.6 step: 20\n",
      "Run: 57, exploration: 0.62, score: -10. step: 20\n",
      "Run: 58, exploration: 0.61, score: -7.6 step: 20\n",
      "Run: 59, exploration: 0.61, score: 33.7 step: 12\n",
      "Run: 60, exploration: 0.60, score: 26.6 step: 19\n",
      "model saved\n",
      "Run: 61, exploration: 0.60, score: -5.1 step: 20\n",
      "Run: 62, exploration: 0.59, score: 24.8 step: 19\n",
      "Run: 63, exploration: 0.59, score: 29.0 step: 16\n",
      "Run: 64, exploration: 0.59, score: 32.8 step: 11\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "import os\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "GAMMA = 0.1\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 100000\n",
    "MIN_MEMORY_SIZE = 200\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.9995\n",
    "\n",
    "from environment.environment import GridWorld\n",
    "\n",
    "now = '30x40_8POI'\n",
    "\n",
    "filling = True\n",
    "scores = []\n",
    "averages = []\n",
    "episodes = []\n",
    "\n",
    "debug_mode = False\n",
    "show_graph_every = 1\n",
    "means = False\n",
    "resume = False\n",
    "\n",
    "\n",
    "env = GridWorld(show_graph_every, debug_mode, means)\n",
    "\n",
    "\n",
    "size = 1200\n",
    "\n",
    "\n",
    "\n",
    "class DQNSolver:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        if resume:\n",
    "            self.model = keras.models.load_model('models/'+now+'.h5', custom_objects=None, compile=True)\n",
    "            print('Model loaded')\n",
    "            self.exploration_rate = EXPLORATION_MAX\n",
    "            self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "            return\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(size, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(size, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "        print('Model created')\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):        \n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def save(self):\n",
    "        self.model.save('models/'+now+'.h5')   \n",
    "        print('model saved')\n",
    "    \n",
    "    def experience_replay(self, run):\n",
    "        if len(self.memory) < MIN_MEMORY_SIZE:\n",
    "            return\n",
    "        global filling\n",
    "        filling = False\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)                \n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "\n",
    "def cartpole():\n",
    "    env = GridWorld(show_graph_every, debug_mode, means)\n",
    "    #observation_space = env.observation_space.shape[0]\n",
    "    action_space = 6\n",
    "    observation_space = size\n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "    run = 0\n",
    "    while True:\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            action = dqn_solver.act(state)\n",
    "            observe_past, reward, total_reward, state_next, terminal = env.step(action)\n",
    "            #reward = reward if not terminal else -reward\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            state = state_next\n",
    "            if terminal or (not filling and step == 20):\n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate)[:4] + \", score: \" + str(total_reward)[:4] + ' step: '+ str(step))\n",
    "                PlotModel(total_reward, run)                \n",
    "                break\n",
    "            dqn_solver.experience_replay(run)\n",
    "        if run % 20 == 0:\n",
    "            dqn_solver.save()\n",
    "\n",
    "\n",
    "\n",
    "def cartpole_test():\n",
    "    #observation_space = env.observation_space.shape[0]\n",
    "    global show_graph_every\n",
    "    show_graph_every = 1\n",
    "    env = GridWorld(show_graph_every, debug_mode, means)\n",
    "    action_space = 5\n",
    "    observation_space = size\n",
    "    model = keras.models.load_model('models/'+now+'.h5', custom_objects=None, compile=True)\n",
    "    print('Model loaded')\n",
    "    run = 0\n",
    "    while True:\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            action = np.argmax(model.predict(state)[0])\n",
    "            observe_past, reward, total_reward, state_next, terminal = env.step(action)\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                print (\"Test: \" + str(run) + \", score: \" + str(total_reward)[:4] + ' step: '+ str(step))\n",
    "                break            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "def PlotModel(score, episode_number):\n",
    "    scores.append(score)\n",
    "    averages.append(sum(scores[-50:]) / len(scores[-50:]))\n",
    "    episodes.append(episode_number)\n",
    "    pylab.plot(episodes, scores, 'b')\n",
    "    pylab.plot(episodes, averages, 'r')\n",
    "    pylab.ylabel('Score', fontsize=18)\n",
    "    pylab.xlabel('Games', fontsize=18)\n",
    "    file_name = \"breakout_model_{}\".format(now)\n",
    "    name = file_name + '.png'\n",
    "    try:\n",
    "        if not os.path.exists('training_images'): os.makedirs('training_images')\n",
    "        pylab.savefig('training_images/'+name)\n",
    "\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    return    \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cartpole()\n",
    "    #cartpole_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
