{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "WARNING:tensorflow:From /home/lorenzo/anaconda3/envs/lorenzo_env/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from environment import GridWorld\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from keras import layers\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from keras.models import Model\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "from collections import deque\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras.models import clone_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pylab\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "\n",
    "\n",
    "debug_mode = False\n",
    "show_graph_every = 1\n",
    "means = False\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', 'tf_train_breakout',\n",
    "                           \"\"\"Directory where to write event logs and checkpoint. \"\"\")\n",
    "tf.app.flags.DEFINE_string('restore_file_path',\n",
    "                           'tf_train_breakout/breakout_model_testGridWordl1',\n",
    "                           \"\"\"Path of the restore file \"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_episode', 100000,\n",
    "                            \"\"\"number of epochs of the optimization loop.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('observe_step_num', 1,\n",
    "                            \"\"\"Timesteps to observe before training.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('epsilon_step_num', 1000000,\n",
    "                            \"\"\"frames over which to anneal epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('refresh_target_model_num', 10000,  # update the target Q model every refresh_target_model_num\n",
    "                            \"\"\"frames over which to anneal epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('replay_memory', 400000,  # takes up to 20 GB to store this amount of history data\n",
    "                            \"\"\"number of previous transitions to remember.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('no_op_steps', 1,\n",
    "                            \"\"\"Number of the steps that runs before script begin.\"\"\")\n",
    "tf.app.flags.DEFINE_float('regularizer_scale', 0.01,\n",
    "                          \"\"\"L1 regularizer scale.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32,\n",
    "                            \"\"\"Size of minibatch to train.\"\"\")\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.00025,\n",
    "                          \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_float('init_epsilon', 1.0,\n",
    "                          \"\"\"starting value of epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_float('final_epsilon', 0.1,\n",
    "                          \"\"\"final value of epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_float('gamma', 0.5,\n",
    "                          \"\"\"decay rate of past observations.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('resume', True,\n",
    "                            \"\"\"Whether to resume from previous checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('render', False,\n",
    "                            \"\"\"Whether to display the game.\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lorenzo/anaconda3/envs/lorenzo_env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0404 02:22:01.718569 139786272495424 deprecation.py:506] From /home/lorenzo/anaconda3/envs/lorenzo_env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "move right\n",
      "move up\n",
      "move down\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-066b1bc3172b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lorenzo_env/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_parse_flags_tolerate_undef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-066b1bc3172b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;31m#test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-066b1bc3172b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# check if the memory is ready for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve_step_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-066b1bc3172b>\u001b[0m in \u001b[0;36mtrain_memory_batch\u001b[0;34m(memory, model, log_dir)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# train model by random batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     history = np.zeros((FLAGS.batch_size, ATARI_SHAPE[0],\n\u001b[1;32m     88\u001b[0m                         ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
      "\u001b[0;32m~/anaconda3/envs/lorenzo_env/lib/python3.6/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m21\u001b[0m        \u001b[0;31m# size of a small set minus size of an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ATARI_SHAPE = (8, 5, 4)  # input image size to model\n",
    "ACTION_SIZE = 7\n",
    "\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "now = 'testGridWordl2'\n",
    "\n",
    "scores = []\n",
    "averages = []\n",
    "episodes = []\n",
    "\n",
    "# 210*160*3(color) --> 84*84(mono)\n",
    "# float --> integer (to reduce the size of replay memory)\n",
    "def pre_processing(observe):\n",
    "    return observe\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe\n",
    "\n",
    "\n",
    "def huber_loss(y, q_value):\n",
    "    error = K.abs(y - q_value)\n",
    "    quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "    linear_part = error - quadratic_part\n",
    "    loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def atari_model():\n",
    "    # With the functional API we need to define the inputs.\n",
    "    frames_input = layers.Input(ATARI_SHAPE, name='frames')\n",
    "    actions_input = layers.Input((ACTION_SIZE,), name='action_mask')\n",
    "\n",
    "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "    normalized = layers.Lambda(lambda x: x / 2, name='normalization')(frames_input)\n",
    "    print(normalized)\n",
    "    \n",
    "    '''\n",
    "    # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "    conv_1 = layers.convolutional.Conv2D(\n",
    "        1, (2, 2), strides=(4, 4), activation='relu'\n",
    "    )(normalized)\n",
    "    # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "    conv_2 = layers.convolutional.Conv2D(\n",
    "        32, (4, 4), strides=(2, 2), activation='relu'\n",
    "    )(conv_1)\n",
    "    '''\n",
    "    # Flattening the second convolutional layer.\n",
    "    conv_flattened = layers.core.Flatten()(normalized)\n",
    "    # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "    hidden = layers.Dense(256, activation='relu')(conv_flattened)\n",
    "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "    output = layers.Dense(ACTION_SIZE)(hidden)\n",
    "    # Finally, we multiply the output by the mask!\n",
    "    filtered_output = layers.Multiply(name='QValue')([output, actions_input])\n",
    "\n",
    "    model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "    model.summary()\n",
    "    optimizer = RMSprop(lr=FLAGS.learning_rate, rho=0.95, epsilon=0.01)\n",
    "    # model.compile(optimizer, loss='mse')\n",
    "    # to changed model weights more slowly, uses MSE for low values and MAE(Mean Absolute Error) for large values\n",
    "    model.compile(optimizer, loss=huber_loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "# get action from model using epsilon-greedy policy\n",
    "def get_action(history, epsilon, step, model):\n",
    "    if np.random.rand() <= epsilon or step <= FLAGS.observe_step_num:\n",
    "        return random.randrange(ACTION_SIZE)\n",
    "    else:\n",
    "        q_value = model.predict([history, np.ones(ACTION_SIZE).reshape(1, ACTION_SIZE)])\n",
    "        return np.argmax(q_value[0])\n",
    "\n",
    "\n",
    "# save sample <s,a,r,s'> to the replay memory\n",
    "def store_memory(memory, history, action, reward, next_history):\n",
    "    memory.append((history, action, reward, next_history))\n",
    "\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "\n",
    "\n",
    "# train model by random batch\n",
    "def train_memory_batch(memory, model, log_dir):\n",
    "    mini_batch = random.sample(memory, FLAGS.batch_size)\n",
    "    history = np.zeros((FLAGS.batch_size, ATARI_SHAPE[0],\n",
    "                        ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    next_history = np.zeros((FLAGS.batch_size, ATARI_SHAPE[0],\n",
    "                             ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    target = np.zeros((FLAGS.batch_size,))\n",
    "    action, reward = [], []\n",
    "\n",
    "    for idx, val in enumerate(mini_batch):\n",
    "        history[idx] = val[0]\n",
    "        next_history[idx] = val[3]\n",
    "        action.append(val[1])\n",
    "        reward.append(val[2])\n",
    "\n",
    "    actions_mask = np.ones((FLAGS.batch_size, ACTION_SIZE))\n",
    "    next_Q_values = model.predict([next_history, actions_mask])\n",
    "\n",
    "    # like Q Learning, get maximum Q value at s'\n",
    "    # But from target model\n",
    "    for i in range(FLAGS.batch_size):\n",
    "        target[i] = reward[i] + FLAGS.gamma * np.amax(next_Q_values[i])\n",
    "\n",
    "    action_one_hot = get_one_hot(action, ACTION_SIZE)\n",
    "    target_one_hot = action_one_hot * target[:, None]\n",
    "\n",
    "    #tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=0,\n",
    "    #                          write_graph=True, write_images=False)\n",
    "\n",
    "    ''''''\n",
    "    h = model.fit(\n",
    "        [history, action_one_hot], target_one_hot, epochs=1,\n",
    "        batch_size=FLAGS.batch_size, verbose=0)\n",
    "        #batch_size=FLAGS.batch_size, verbose=0, callbacks=[tb_callback])\n",
    "\n",
    "    #if h.history['loss'][0] > 10.0:\n",
    "    #    print('too large')\n",
    "\n",
    "    return h.history['loss'][0]\n",
    "\n",
    "def train():\n",
    "    \n",
    "    \n",
    "    env = GridWorld(show_graph_every, debug_mode, means)\n",
    "    \n",
    "    \n",
    "    # deque: Once a bounded length deque is full, when new items are added,\n",
    "    # a corresponding number of items are discarded from the opposite end\n",
    "    memory = deque(maxlen=FLAGS.replay_memory)\n",
    "    episode_number = 0\n",
    "    epsilon = FLAGS.init_epsilon\n",
    "    epsilon_decay = (FLAGS.init_epsilon - FLAGS.final_epsilon) / FLAGS.epsilon_step_num\n",
    "    global_step = 0\n",
    "\n",
    "    \n",
    "    if FLAGS.resume:\n",
    "        \n",
    "        # load json and create model\n",
    "        json_file = open(FLAGS.restore_file_path+'.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        model.load_weights(FLAGS.restore_file_path+'.h5')\n",
    "        \n",
    "        optimizer = RMSprop(lr=FLAGS.learning_rate, rho=0.95, epsilon=0.01)\n",
    "        # model.compile(optimizer, loss='mse')\n",
    "        # to changed model weights more slowly, uses MSE for low values and MAE(Mean Absolute Error) for large values\n",
    "        model.compile(optimizer, loss=huber_loss)\n",
    "        \n",
    "        print(\"Loaded model from disk\")\n",
    "        '''\n",
    "        model = load_model('tf_train_breakout/breakout_model_20180610205843_36h_12193ep_sec_version_back.h5')#, custom_objects={'huber_loss': huber_loss})\n",
    "        print(\"Loaded model from disk\")\n",
    "        #model = load_model(FLAGS.restore_file_path)\n",
    "        '''\n",
    "        #model = load_model(FLAGS.restore_file_path)\n",
    "        # Assume when we restore the model, the epsilon has already decreased to the final value\n",
    "        epsilon = FLAGS.final_epsilon\n",
    "    else:\n",
    "        model = atari_model()\n",
    "\n",
    "    log_dir = \"logs/{}/run-{}-log\".format(FLAGS.train_dir, now)\n",
    "    file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "    model_target = clone_model(model)\n",
    "    model_target.set_weights(model.get_weights())\n",
    "\n",
    "    while episode_number < FLAGS.num_episode:\n",
    "\n",
    "        done = False\n",
    "        # 1 episode = 5 lives\n",
    "        step, score = 0, 0\n",
    "        loss = 0.0\n",
    "        observe = env.reset()\n",
    "\n",
    "        # this is one of DeepMind's idea.\n",
    "        # just do nothing at the start of episode to avoid sub-optimal\n",
    "        for _ in range(random.randint(1, FLAGS.no_op_steps)):\n",
    "            observe, _, _, _, _ = env.step(1)\n",
    "        # At start of episode, there is no preceding frame\n",
    "        # So just copy initial states to make history\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 8, 5, 4))\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # get action for the current history and go one step in environment\n",
    "            action = get_action(history, epsilon, global_step, model_target)\n",
    "            # change action to real_action\n",
    "            real_action = action + 1\n",
    "\n",
    "            # scale down epsilon, the epsilon only begin to decrease after observe steps\n",
    "            if epsilon > FLAGS.final_epsilon and global_step > FLAGS.observe_step_num:\n",
    "                epsilon -= epsilon_decay\n",
    "\n",
    "            #observe, reward, done, info = env.step(real_action)\n",
    "            observe, reward, total_reward, next_state, done = env.step(real_action)\n",
    "\n",
    "            \n",
    "            # pre-process the observation --> history\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 8, 5, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "\n",
    "            # save the statue to memory, each replay takes 2 * (84*84*4) bytes = 56448 B = 55.125 KB\n",
    "            store_memory(memory, history, action, reward, next_history)  #\n",
    "\n",
    "            # check if the memory is ready for training\n",
    "            if global_step > FLAGS.observe_step_num:\n",
    "                loss = loss + train_memory_batch(memory, model, log_dir)\n",
    "                print(memory.shape)\n",
    "                print(memory)\n",
    "                # if loss > 100.0:\n",
    "                #    print(loss)\n",
    "                if global_step % FLAGS.refresh_target_model_num == 0:  # update the target model\n",
    "                    model_target.set_weights(model.get_weights())\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            #print(\"step: \", global_step)\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                if global_step <= FLAGS.observe_step_num:\n",
    "                    state = \"observe\"\n",
    "                elif FLAGS.observe_step_num < global_step <= FLAGS.observe_step_num + FLAGS.epsilon_step_num:\n",
    "                    state = \"explore\"\n",
    "                else:\n",
    "                    state = \"train\"\n",
    "                print('state: {}, episode: {}, score: {}, global_step: {}, avg loss: {}, step: {}, memory length: {}'\n",
    "                      .format(state, episode_number, score, global_step, loss / float(step), step, len(memory)))\n",
    "\n",
    "                if episode_number % 100 == 0 or (episode_number + 1) == FLAGS.num_episode:\n",
    "        \n",
    "                #if episode_number % 1 == 0 or (episode_number + 1) == FLAGS.num_episode:  # debug\n",
    "                    file_name = \"breakout_model_{}\".format(now)\n",
    "                    model_path = os.path.join(FLAGS.train_dir, file_name)\n",
    "                    \n",
    "                    # serialize model to JSON\n",
    "                    model_json = model.to_json()\n",
    "                    with open(model_path+'.json', \"w\") as json_file:\n",
    "                        json_file.write(model_json)\n",
    "                    # serialize weights to HDF5\n",
    "                    model.save_weights(model_path+'.h5')\n",
    "                    #model.save(model_path)\n",
    "\n",
    "                # Add user custom data to TensorBoard\n",
    "                loss_summary = tf.Summary(\n",
    "                    value=[tf.Summary.Value(tag=\"loss\", simple_value=loss / float(step))])\n",
    "                file_writer.add_summary(loss_summary, global_step=episode_number)\n",
    "\n",
    "                score_summary = tf.Summary(\n",
    "                    value=[tf.Summary.Value(tag=\"score\", simple_value=score)])\n",
    "                file_writer.add_summary(score_summary, global_step=episode_number)\n",
    "\n",
    "                episode_number += 1\n",
    "                PlotModel(score, episode_number)\n",
    "\n",
    "\n",
    "    file_writer.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PlotModel(score, episode_number):\n",
    "    scores.append(score)\n",
    "    averages.append(sum(scores[-50:]) / len(scores[-50:]))\n",
    "    episodes.append(episode_number)\n",
    "    pylab.plot(episodes, scores, 'b')\n",
    "    pylab.plot(episodes, averages, 'r')\n",
    "    pylab.ylabel('Score', fontsize=18)\n",
    "    pylab.xlabel('Games', fontsize=18)\n",
    "    file_name = \"breakout_model_{}\".format(now)\n",
    "    name = file_name + '.png'\n",
    "    try:\n",
    "        if not os.path.exists('training_images'): os.makedirs('training_images')\n",
    "        pylab.savefig('training_images/'+name)\n",
    "\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    return             \n",
    "                \n",
    "     \n",
    "                \n",
    "                \n",
    "                \n",
    "def main(argv=None):\n",
    "    train()\n",
    "    #test()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lorenzo_env] *",
   "language": "python",
   "name": "conda-env-lorenzo_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
