{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from environment import GridWorld\n",
    "import cv2\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from keras import layers\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from keras.models import Model\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "from collections import deque\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "from keras.models import load_model\n",
    "from keras.models import clone_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pylab\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "\n",
    "\n",
    "debug_mode = False\n",
    "show_graph_every = 100\n",
    "means = True\n",
    "n_frames = 2\n",
    "manual = False\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', 'tf_train_breakout',\n",
    "                           \"\"\"Directory where to write event logs and checkpoint. \"\"\")\n",
    "tf.app.flags.DEFINE_string('restore_file_path',\n",
    "                           'tf_train_breakout/breakout_model_breakout_scratch',\n",
    "                           #'tf_train_breakout/breakout_model_testGridWordl1',\n",
    "                           \"\"\"Path of the restore file \"\"\")\n",
    "tf.app.flags.DEFINE_integer('num_episode', 100000,\n",
    "                            \"\"\"number of epochs of the optimization loop.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('observe_step_num', 1,\n",
    "                            \"\"\"Timesteps to observe before training.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('epsilon_step_num', 1000000,\n",
    "                            \"\"\"frames over which to anneal epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('refresh_target_model_num', 10000,  # update the target Q model every refresh_target_model_num\n",
    "                            \"\"\"frames over which to anneal epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('replay_memory', 100000,  # takes up to 20 GB to store this amount of history data\n",
    "                            \"\"\"number of previous transitions to remember.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('no_op_steps', 1,\n",
    "                            \"\"\"Number of the steps that runs before script begin.\"\"\")\n",
    "tf.app.flags.DEFINE_float('regularizer_scale', 0.01,\n",
    "                          \"\"\"L1 regularizer scale.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64,\n",
    "                            \"\"\"Size of minibatch to train.\"\"\")\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.0025,\n",
    "                          \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_float('init_epsilon', 1.0,\n",
    "                          \"\"\"starting value of epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_float('final_epsilon', 0.1,\n",
    "                          \"\"\"final value of epsilon.\"\"\")\n",
    "tf.app.flags.DEFINE_float('gamma', 0.01,\n",
    "                          \"\"\"decay rate of past observations.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('resume', False,\n",
    "                            \"\"\"Whether to resume from previous checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('render', False,\n",
    "                            \"\"\"Whether to display the game.\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if manual:\n",
    "    cv2.namedWindow('GUI')\n",
    "\n",
    "ATARI_SHAPE = (5, 8, n_frames)  # input image size to model\n",
    "ACTION_SIZE = 5\n",
    "\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "now = 'testGridWordl2'\n",
    "\n",
    "scores = []\n",
    "averages = []\n",
    "episodes = []\n",
    "\n",
    "# 210*160*3(color) --> 84*84(mono)\n",
    "# float --> integer (to reduce the size of replay memory)\n",
    "def pre_processing(observe):\n",
    "    return observe\n",
    "\n",
    "\n",
    "def huber_loss(y, q_value):\n",
    "    error = K.abs(y - q_value)\n",
    "    quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "    linear_part = error - quadratic_part\n",
    "    loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def atari_model():\n",
    "    # With the functional API we need to define the inputs.\n",
    "    frames_input = layers.Input(ATARI_SHAPE, name='frames')\n",
    "    actions_input = layers.Input((ACTION_SIZE,), name='action_mask')\n",
    "    '''\n",
    "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "    #normalized = layers.Lambda(lambda x: x / 2, name='normalization')(frames_input)\n",
    "    \n",
    "    # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "    conv_1 = layers.convolutional.Conv2D(\n",
    "        32, (2, 2), strides=(1, 1), activation='relu'\n",
    "    )(frames_input)\n",
    "\n",
    "    # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "    conv_2 = layers.convolutional.Conv2D(\n",
    "        32, (2, 2), strides=(2, 2), activation='relu'\n",
    "    )(conv_1)\n",
    "    '''\n",
    "    \n",
    "    # Flattening the second convolutional layer.\n",
    "    conv_flattened = layers.core.Flatten()(frames_input)\n",
    "    # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "    hidden = layers.Dense(256, activation='relu')(conv_flattened)\n",
    "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "    output = layers.Dense(ACTION_SIZE)(hidden)\n",
    "    # Finally, we multiply the output by the mask!\n",
    "    filtered_output = layers.Multiply(name='QValue')([output, actions_input])\n",
    "\n",
    "    model = Model(inputs=[frames_input, actions_input], outputs=filtered_output)\n",
    "    model.summary()\n",
    "    optimizer = RMSprop(lr=FLAGS.learning_rate, rho=0.95, epsilon=0.01)\n",
    "    # model.compile(optimizer, loss='mse')\n",
    "    # to changed model weights more slowly, uses MSE for low values and MAE(Mean Absolute Error) for large values\n",
    "    model.compile(optimizer, loss=huber_loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# get action from model using epsilon-greedy policy\n",
    "def get_action(history, epsilon, step, model):\n",
    "    '''\n",
    "    print(history.shape)\n",
    "    print('-------------------------------------------------------------')\n",
    "    objecto = np.reshape(history[0,:,:,0], (5, 8))\n",
    "    print(objecto)\n",
    "    objecto = np.reshape(history[0,:,:,1], (5, 8))\n",
    "    print(objecto)\n",
    "    '''\n",
    "    \n",
    "    if np.random.rand() <= epsilon or step <= FLAGS.observe_step_num:\n",
    "        if manual:\n",
    "            key = cv2.waitKey(0)\n",
    "            if key == ord('w'):\n",
    "                return 4\n",
    "            elif key == ord('s'):\n",
    "                return 3\n",
    "            elif key == ord('a'):\n",
    "                return 2\n",
    "            elif key == ord('d'):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return random.randrange(ACTION_SIZE)\n",
    "    else:\n",
    "        q_value = model.predict([history, np.ones(ACTION_SIZE).reshape(1, ACTION_SIZE)])\n",
    "        return np.argmax(q_value[0])\n",
    "\n",
    "\n",
    "# save sample <s,a,r,s'> to the replay memory\n",
    "def store_memory(memory, history, action, reward, next_history):\n",
    "    memory.append((history, action, reward, next_history))\n",
    "\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "\n",
    "\n",
    "# train model by random batch\n",
    "def train_memory_batch(memory, model, log_dir):\n",
    "    mini_batch = random.sample(memory, FLAGS.batch_size)\n",
    "    history = np.zeros((FLAGS.batch_size, ATARI_SHAPE[0],\n",
    "                        ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    next_history = np.zeros((FLAGS.batch_size, ATARI_SHAPE[0],\n",
    "                             ATARI_SHAPE[1], ATARI_SHAPE[2]))\n",
    "    target = np.zeros((FLAGS.batch_size,))\n",
    "    action, reward = [], []\n",
    "\n",
    "    for idx, val in enumerate(mini_batch):\n",
    "        history[idx] = val[0]\n",
    "        next_history[idx] = val[3]\n",
    "        action.append(val[1])\n",
    "        reward.append(val[2])\n",
    "\n",
    "    actions_mask = np.ones((FLAGS.batch_size, ACTION_SIZE))\n",
    "    next_Q_values = model.predict([next_history, actions_mask])\n",
    "    print(next_Q_values)\n",
    "\n",
    "    # like Q Learning, get maximum Q value at s'\n",
    "    # But from target model\n",
    "    for i in range(FLAGS.batch_size):\n",
    "        target[i] = reward[i] + FLAGS.gamma * np.amax(next_Q_values[i])\n",
    "\n",
    "    action_one_hot = get_one_hot(action, ACTION_SIZE)\n",
    "    target_one_hot = action_one_hot * target[:, None]\n",
    "\n",
    "    #tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=0,\n",
    "    #                          write_graph=True, write_images=False)\n",
    "\n",
    "    ''''''\n",
    "    h = model.fit(\n",
    "        [history, action_one_hot], target_one_hot, epochs=1,\n",
    "        batch_size=FLAGS.batch_size, verbose=0)\n",
    "        #batch_size=FLAGS.batch_size, verbose=0, callbacks=[tb_callback])\n",
    "\n",
    "    #if h.history['loss'][0] > 10.0:\n",
    "    #    print('too large')\n",
    "\n",
    "    return h.history['loss'][0]\n",
    "\n",
    "def train():\n",
    "    \n",
    "    \n",
    "    env = GridWorld(show_graph_every, debug_mode, means)\n",
    "    \n",
    "    \n",
    "    # deque: Once a bounded length deque is full, when new items are added,\n",
    "    # a corresponding number of items are discarded from the opposite end\n",
    "    memory = deque(maxlen=FLAGS.replay_memory)\n",
    "    episode_number = 0\n",
    "    epsilon = FLAGS.init_epsilon\n",
    "    epsilon_decay = (FLAGS.init_epsilon - FLAGS.final_epsilon) / FLAGS.epsilon_step_num\n",
    "    global_step = 0\n",
    "\n",
    "    \n",
    "    if FLAGS.resume:\n",
    "        \n",
    "        # load json and create model\n",
    "        json_file = open(FLAGS.restore_file_path+'.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        model.load_weights(FLAGS.restore_file_path+'.h5')\n",
    "        \n",
    "        optimizer = RMSprop(lr=FLAGS.learning_rate, rho=0.95, epsilon=0.01)\n",
    "        # model.compile(optimizer, loss='mse')\n",
    "        # to changed model weights more slowly, uses MSE for low values and MAE(Mean Absolute Error) for large values\n",
    "        model.compile(optimizer, loss=huber_loss)\n",
    "        \n",
    "        print(\"Loaded model from disk\")\n",
    "        '''\n",
    "        model = load_model('tf_train_breakout/breakout_model_20180610205843_36h_12193ep_sec_version_back.h5')#, custom_objects={'huber_loss': huber_loss})\n",
    "        print(\"Loaded model from disk\")\n",
    "        #model = load_model(FLAGS.restore_file_path)\n",
    "        '''\n",
    "        #model = load_model(FLAGS.restore_file_path)\n",
    "        # Assume when we restore the model, the epsilon has already decreased to the final value\n",
    "        epsilon = FLAGS.final_epsilon\n",
    "    else:\n",
    "        model = atari_model()\n",
    "\n",
    "    log_dir = \"logs/{}/run-{}-log\".format(FLAGS.train_dir, now)\n",
    "    file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "    model_target = clone_model(model)\n",
    "    model_target.set_weights(model.get_weights())\n",
    "\n",
    "    while episode_number < FLAGS.num_episode:\n",
    "\n",
    "        done = False\n",
    "        # 1 episode = 5 lives\n",
    "        step, score = 0, 0\n",
    "        loss = 0.0\n",
    "        observe = env.reset() \n",
    "        '''\n",
    "        # this is one of DeepMind's idea.\n",
    "        # just do nothing at the start of episode to avoid sub-optimal\n",
    "        for _ in range(random.randint(1, FLAGS.no_op_steps)):\n",
    "            observe, _, _, _, _ = env.step(1)\n",
    "        '''\n",
    "        # At start of episode, there is no preceding frame\n",
    "        # So just copy initial states to make history\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 5, 8, n_frames))\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # get action for the current history and go one step in environment\n",
    "            action = get_action(history, epsilon, global_step, model_target)\n",
    "            # change action to real_action\n",
    "            real_action = action #+ 1\n",
    "\n",
    "            # scale down epsilon, the epsilon only begin to decrease after observe steps\n",
    "            if epsilon > FLAGS.final_epsilon and global_step > FLAGS.observe_step_num:\n",
    "                epsilon -= epsilon_decay\n",
    "\n",
    "            #observe, reward, done, info = env.step(real_action)\n",
    "            observe_past, reward, total_reward, observe, done = env.step(real_action)\n",
    "            # pre-process the observation --> history\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 5, 8, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :n_frames-1], axis=3)\n",
    "            \n",
    "            # save the statue to memory, each replay takes 2 * (84*84*4) bytes = 56448 B = 55.125 KB\n",
    "            store_memory(memory, history, action, reward, next_history)  #\n",
    "\n",
    "            # check if the memory is ready for training\n",
    "            if global_step > FLAGS.observe_step_num:\n",
    "                loss = loss + train_memory_batch(memory, model, log_dir)\n",
    "                # if loss > 100.0:\n",
    "                #    print(loss)\n",
    "                if global_step % FLAGS.refresh_target_model_num == 0:  # update the target model\n",
    "                    model_target.set_weights(model.get_weights())\n",
    "\n",
    "            score += reward\n",
    "            history = next_history\n",
    "\n",
    "            #print(\"step: \", global_step)\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            if done:\n",
    "                if global_step <= FLAGS.observe_step_num:\n",
    "                    state = \"observe\"\n",
    "                elif FLAGS.observe_step_num < global_step <= FLAGS.observe_step_num + FLAGS.epsilon_step_num:\n",
    "                    state = \"explore\"\n",
    "                else:\n",
    "                    state = \"train\"\n",
    "                print('state: {}, episode: {}, score: {}, global_step: {}, avg loss: {}, step: {}, memory length: {}'\n",
    "                      .format(state, episode_number, score, global_step, loss / float(step), step, len(memory)))\n",
    "\n",
    "                if episode_number % 100 == 0 or (episode_number + 1) == FLAGS.num_episode:\n",
    "        \n",
    "                #if episode_number % 1 == 0 or (episode_number + 1) == FLAGS.num_episode:  # debug\n",
    "                    file_name = \"breakout_model_{}\".format(now)\n",
    "                    model_path = os.path.join(FLAGS.train_dir, file_name)\n",
    "                    \n",
    "                    # serialize model to JSON\n",
    "                    model_json = model.to_json()\n",
    "                    with open(model_path+'.json', \"w\") as json_file:\n",
    "                        json_file.write(model_json)\n",
    "                    # serialize weights to HDF5\n",
    "                    model.save_weights(model_path+'.h5')\n",
    "                    #model.save(model_path)\n",
    "\n",
    "                # Add user custom data to TensorBoard\n",
    "                loss_summary = tf.Summary(\n",
    "                    value=[tf.Summary.Value(tag=\"loss\", simple_value=loss / float(step))])\n",
    "                file_writer.add_summary(loss_summary, global_step=episode_number)\n",
    "\n",
    "                score_summary = tf.Summary(\n",
    "                    value=[tf.Summary.Value(tag=\"score\", simple_value=score)])\n",
    "                file_writer.add_summary(score_summary, global_step=episode_number)\n",
    "\n",
    "                episode_number += 1\n",
    "                PlotModel(score, episode_number)\n",
    "\n",
    "    file_writer.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PlotModel(score, episode_number):\n",
    "    scores.append(score)\n",
    "    averages.append(sum(scores[-50:]) / len(scores[-50:]))\n",
    "    episodes.append(episode_number)\n",
    "    pylab.plot(episodes, scores, 'b')\n",
    "    pylab.plot(episodes, averages, 'r')\n",
    "    pylab.ylabel('Score', fontsize=18)\n",
    "    pylab.xlabel('Games', fontsize=18)\n",
    "    file_name = \"breakout_model_{}\".format(now)\n",
    "    name = file_name + '.png'\n",
    "    try:\n",
    "        if not os.path.exists('training_images'): os.makedirs('training_images')\n",
    "        pylab.savefig('training_images/'+name)\n",
    "\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    return             \n",
    "                \n",
    "     \n",
    "                \n",
    "                \n",
    "                \n",
    "def main(argv=None):\n",
    "    train()\n",
    "    #test()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from scores.score_logger import ScoreLogger\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "\n",
    "\n",
    "class DQNSolver:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "\n",
    "def cartpole():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    score_logger = ScoreLogger(ENV_NAME)\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "    run = 0\n",
    "    while True:\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            #env.render()\n",
    "            action = dqn_solver.act(state)\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            reward = reward if not terminal else -reward\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                print \"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step)\n",
    "                score_logger.add_score(step, run)\n",
    "                break\n",
    "            dqn_solver.experience_replay()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "import os\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "GAMMA = 0.1\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 100000\n",
    "MIN_MEMORY_SIZE = 2000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.9995\n",
    "\n",
    "from environment import GridWorld\n",
    "\n",
    "now = '30x40_randomAgent_fixedTarget_nomeans'\n",
    "\n",
    "scores = []\n",
    "averages = []\n",
    "episodes = []\n",
    "\n",
    "debug_mode = False\n",
    "show_graph_every = 100\n",
    "means = False\n",
    "resume = True\n",
    "\n",
    "\n",
    "env = GridWorld(show_graph_every, debug_mode, means)\n",
    "\n",
    "\n",
    "size = 1200\n",
    "\n",
    "\n",
    "\n",
    "class DQNSolver:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        if resume:\n",
    "            self.model = keras.models.load_model('models/'+now+'.h5', custom_objects=None, compile=True)\n",
    "            print('Model loaded')\n",
    "            self.exploration_rate = EXPLORATION_MIN\n",
    "            self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "            return\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(size, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(size, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n",
    "        print('Model created')\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):        \n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def save(self):\n",
    "        self.model.save('models/'+now+'.h5')   \n",
    "        print('model saved')\n",
    "    \n",
    "    def experience_replay(self, run):\n",
    "        if len(self.memory) < MIN_MEMORY_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)                \n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "\n",
    "def cartpole():\n",
    "    env = GridWorld(show_graph_every, debug_mode, means)\n",
    "    #observation_space = env.observation_space.shape[0]\n",
    "    action_space = 6\n",
    "    observation_space = size\n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "    run = 0\n",
    "    while True:\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            action = dqn_solver.act(state)\n",
    "            observe_past, reward, total_reward, state_next, terminal = env.step(action)\n",
    "            #reward = reward if not terminal else -reward\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate)[:4] + \", score: \" + str(total_reward)[:4] + ' step: '+ str(step))\n",
    "                PlotModel(total_reward, run)                \n",
    "                break\n",
    "            dqn_solver.experience_replay(run)\n",
    "        if run % 20 == 0:\n",
    "            dqn_solver.save()\n",
    "\n",
    "\n",
    "\n",
    "def cartpole_test():\n",
    "    #observation_space = env.observation_space.shape[0]\n",
    "    global show_graph_every\n",
    "    show_graph_every = 1\n",
    "    env = GridWorld(show_graph_every, debug_mode, means)\n",
    "    action_space = 5\n",
    "    observation_space = size\n",
    "    model = keras.models.load_model('models/'+now+'.h5', custom_objects=None, compile=True)\n",
    "    print('Model loaded')\n",
    "    run = 0\n",
    "    while True:\n",
    "        run += 1\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_space])\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            action = np.argmax(model.predict(state)[0])\n",
    "            observe_past, reward, total_reward, state_next, terminal = env.step(action)\n",
    "            state_next = np.reshape(state_next, [1, observation_space])\n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                print (\"Test: \" + str(run) + \", score: \" + str(total_reward)[:4] + ' step: '+ str(step))\n",
    "                break\n",
    "         \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "def PlotModel(score, episode_number):\n",
    "    scores.append(score)\n",
    "    averages.append(sum(scores[-50:]) / len(scores[-50:]))\n",
    "    episodes.append(episode_number)\n",
    "    pylab.plot(episodes, scores, 'b')\n",
    "    pylab.plot(episodes, averages, 'r')\n",
    "    pylab.ylabel('Score', fontsize=18)\n",
    "    pylab.xlabel('Games', fontsize=18)\n",
    "    file_name = \"breakout_model_{}\".format(now)\n",
    "    name = file_name + '.png'\n",
    "    try:\n",
    "        if not os.path.exists('training_images'): os.makedirs('training_images')\n",
    "        pylab.savefig('training_images/'+name)\n",
    "\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    return    \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #cartpole()\n",
    "    cartpole_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
