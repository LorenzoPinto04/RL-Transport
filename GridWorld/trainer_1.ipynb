{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__ is 2.1.0\n",
      "tf.keras.__version__ is: 2.2.4-tf\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "\n",
    "print(\"tf.__version__ is\", tf.__version__)\n",
    "print(\"tf.keras.__version__ is:\", tf.keras.__version__)\n",
    "\n",
    "def _get_available_gpus():\n",
    "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
    "\n",
    "    # Returns\n",
    "        A list of available GPU devices.\n",
    "    \"\"\"\n",
    "    #global _LOCAL_DEVICES\n",
    "    if tfback._LOCAL_DEVICES is None:\n",
    "        devices = tf.config.list_logical_devices()\n",
    "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "\n",
    "tfback._get_available_gpus = _get_available_gpus\n",
    "\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lorenzo/anaconda3/envs/lorenzo_env/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pylab\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "from environment import GridWorld\n",
    "\n",
    "\n",
    "def OurModel(input_shape, action_space, dueling):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    #X = Conv2D(64, 5, strides=(3, 3),padding=\"valid\", input_shape=input_shape, activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(32, 8, strides=(4, 4),padding=\"valid\", input_shape=input_shape, activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(64, 4, strides=(2, 2),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(64, 3, strides=(1, 1),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Flatten()(X)\n",
    "    \n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    X = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    if dueling:\n",
    "        state_value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "        state_value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(action_space,))(state_value)\n",
    "\n",
    "        action_advantage = Dense(action_space, kernel_initializer='he_uniform')(X)\n",
    "        action_advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(action_space,))(action_advantage)\n",
    "\n",
    "        X = Add()([state_value, action_advantage])\n",
    "    else:\n",
    "        # Output Layer with # of actions: 2 nodes (left, right)\n",
    "        X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    #model.compile(loss=\"mean_squared_error\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "    #model.compile(optimizer=Adam(lr=0.00025), loss='mean_squared_error')\n",
    "    model.compile(optimizer=Adam(lr=0.00005), loss='mean_squared_error')\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env_name, env):\n",
    "        self.env_name = env_name       \n",
    "        self.env = env\n",
    "        self.action_size = 8\n",
    "        self.EPISODES = 1000\n",
    "        \n",
    "        # Instantiate memory\n",
    "        memory_size = 25000\n",
    "        self.MEMORY = Memory(memory_size)\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        \n",
    "        # EXPLORATION HYPERPARAMETERS for epsilon and epsilon greedy strategy\n",
    "        self.epsilon = 1.0  # exploration probability at start\n",
    "        self.epsilon_min = 0.02  # minimum exploration probability \n",
    "        self.epsilon_decay = 0.00002  # exponential decay rate for exploration prob\n",
    "        \n",
    "        self.batch_size = 32\n",
    "\n",
    "        # defining model parameters\n",
    "        self.ddqn = False # use doudle deep q network\n",
    "        self.dueling = False # use dealing netowrk\n",
    "        self.epsilon_greedy = False # use epsilon greedy strategy\n",
    "        self.USE_PER = False # use priority experienced replay\n",
    "        \n",
    "\n",
    "        self.Save_Path = 'models'\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name+\"_CNN.h5\")\n",
    "\n",
    "        self.ROWS = 50\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 4\n",
    "        self.update_model_steps = 1000\n",
    "        \n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "        \n",
    "        # create main model and target model\n",
    "        self.model = OurModel(input_shape=self.state_size, action_space = self.action_size, dueling = self.dueling)\n",
    "        self.target_model = OurModel(input_shape=self.state_size, action_space = self.action_size, dueling = self.dueling)  \n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self, game_steps):\n",
    "        if game_steps % self.update_model_steps == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            return\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        experience = state, action, reward, next_state, done\n",
    "        if self.USE_PER:\n",
    "            self.MEMORY.store(experience)\n",
    "        else:\n",
    "            self.memory.append((experience))\n",
    "\n",
    "    def act(self, state, decay_step):\n",
    "        # EPSILON GREEDY STRATEGY\n",
    "        if self.epsilon_greedy:\n",
    "        # Here we'll use an improved version of our epsilon greedy strategy for Q-learning\n",
    "            explore_probability = self.epsilon_min + (self.epsilon - self.epsilon_min) * np.exp(-self.epsilon_decay * decay_step)\n",
    "        # OLD EPSILON STRATEGY\n",
    "        else:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= (1-self.epsilon_decay)\n",
    "            explore_probability = self.epsilon\n",
    "    \n",
    "        if explore_probability > np.random.rand():\n",
    "            # Make a random action (exploration)\n",
    "            return random.randrange(self.action_size), explore_probability\n",
    "        else:\n",
    "            # Get action from Q-network (exploitation)\n",
    "            # Estimate the Qs values state\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            return np.argmax(self.model.predict(state)), explore_probability\n",
    "                \n",
    "    def replay(self):\n",
    "        if self.USE_PER:\n",
    "            # Sample minibatch from the PER memory\n",
    "            tree_idx, minibatch  = self.MEMORY.sample(self.batch_size)\n",
    "        else:\n",
    "            if len(self.memory) > self.batch_size:\n",
    "            # Randomly sample minibatch from the deque memory\n",
    "                minibatch = random.sample(self.memory, self.batch_size)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "        state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        action = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        reward = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        next_state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        done = np.zeros(self.batch_size, dtype=np.uint8)\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop       \n",
    "        for i in range(len(minibatch)):\n",
    "            state[i], action[i], reward[i], next_state[i], done[i] = minibatch[i]\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        # predict Q-values for starting state using the main network\n",
    "        target = self.model.predict(state)\n",
    "        target_old = np.array(target)\n",
    "        # predict best action in ending state using the main network\n",
    "        target_next = self.model.predict(next_state)\n",
    "        # predict Q-values for ending state using the target network\n",
    "        target_val = self.target_model.predict(next_state)\n",
    "\n",
    "\n",
    "        for i in range(len(minibatch)):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                if self.ddqn: # Double - DQN\n",
    "                    # current Q Network selects the action\n",
    "                    # a'_max = argmax_a' Q(s', a')\n",
    "                    a = np.argmax(target_next[i])\n",
    "                    # target Q Network evaluates the action\n",
    "                    # Q_max = Q_target(s', a'_max)\n",
    "                    target[i][action[i]] = reward[i] + self.gamma * target_val[i][a]\n",
    "                else: # Standard - DQN\n",
    "                    # DQN chooses the max Q value among next actions\n",
    "                    # selection and evaluation of action is on the target Q Network\n",
    "                    # Q_max = max_a' Q_target(s', a')\n",
    "                    # when using target model in simple DQN rules, we get better performance\n",
    "                    target[i][action[i]] = reward[i] + self.gamma * np.amax(target_val[i])\n",
    "            \n",
    "        if self.USE_PER:\n",
    "            indices = np.arange(self.batch_size, dtype=np.int32)\n",
    "            absolute_errors = np.abs(target_old[indices, action]-target[indices, action])\n",
    "\n",
    "            # Update priority\n",
    "            self.MEMORY.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name, by_name=False)\n",
    "\n",
    "    def save(self, name):\n",
    "        model = self.model\n",
    "        model.save_weights(name)\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    plt.figure(figsize=(18,9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        pylab.plot(self.episodes, self.average, 'r')\n",
    "        pylab.plot(self.episodes, self.scores, 'b')\n",
    "        pylab.ylabel('Score', fontsize=18)\n",
    "        pylab.xlabel('Games', fontsize=18)\n",
    "        dqn = 'DQN_'\n",
    "        dueling = ''\n",
    "        greedy = ''\n",
    "        PER = ''\n",
    "        if self.ddqn: dqn = '_DDQN'\n",
    "        if self.dueling: dueling = '_Dueling'\n",
    "        if self.epsilon_greedy: greedy = '_Greedy'\n",
    "        if self.USE_PER: PER = '_PER'\n",
    "        try:\n",
    "            if not os.path.exists('training_images'): os.makedirs('training_images')\n",
    "            pylab.savefig('training_images/'+self.env_name+dqn+dueling+greedy+PER+\"_CNN.png\")\n",
    "            \n",
    "        except OSError as e:\n",
    "            pass\n",
    "        # no need to worry about model, when doing a lot of experiments\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name+dqn+dueling+greedy+PER+\"_CNN.h5\")\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(\"cartpole\"+str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def GetImage(self, frame):\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        self.image_memory[0,:,:] = frame\n",
    "\n",
    "        # show image frame   \n",
    "        #self.imshow(self.image_memory,0)\n",
    "        #self.imshow(self.image_memory,1)\n",
    "        #self.imshow(self.image_memory,2)\n",
    "        #self.imshow(self.image_memory,3)\n",
    "        \n",
    "        return np.expand_dims(self.image_memory, axis=0)\n",
    "\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(frame)\n",
    "        return state\n",
    "\n",
    "    def step(self,action):\n",
    "        info = None\n",
    "        state, reward, total_reward, next_state, done = self.env.step(action)\n",
    "        next_state = self.GetImage(next_state)\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def run(self):\n",
    "        decay_step = 0\n",
    "        max_average = -21.0\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            SAVING = ''\n",
    "            while not done:\n",
    "                decay_step += 1\n",
    "                action, explore_probability = self.act(state, decay_step)\n",
    "                next_state, reward, done, _ = self.step(action)\n",
    "                '''\n",
    "                if reward != 0.0:\n",
    "                    print('States -------------------------------------------------------------------------')\n",
    "                    plt.imshow(state[0][0])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][1])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][2])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][3])\n",
    "                    plt.show()\n",
    "                    print('Action:', action)\n",
    "                    print('Reward:', reward)\n",
    "                    '''\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    # every episode, plot the result\n",
    "                    average = self.PlotModel(score, e)\n",
    "\n",
    "                    # saving best models\n",
    "                    if average >= max_average:\n",
    "                        max_average = average\n",
    "                        self.save(self.Model_name)\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2f}, average: {:.2f} {}\".format(e, self.EPISODES, score, explore_probability, average, SAVING))\n",
    "                    \n",
    "                # update target model\n",
    "                self.update_target_model(decay_step)\n",
    "\n",
    "                # train model\n",
    "                self.replay()\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        #self.env.close()\n",
    "\n",
    "    def test(self, Model_name):\n",
    "        import time\n",
    "        self.load(Model_name)\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                time.sleep(.01)\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                state, reward, total_reward, next_state, done = self.env.step(action)\n",
    "                state = self.GetImage(next_state)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "        #self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lorenzo/anaconda3/envs/lorenzo_env/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 4, 50, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 11, 19)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 4, 8)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 2, 6)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 620,008\n",
      "Trainable params: 620,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4, 50, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 11, 19)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 4, 8)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 2, 6)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 620,008\n",
      "Trainable params: 620,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0/1000, score: -113.99999999999991\n",
      "episode: 1/1000, score: -48.999999999999986\n",
      "episode: 2/1000, score: -24.20000000000002\n",
      "episode: 3/1000, score: -141.99999999999994\n",
      "episode: 4/1000, score: -29.90000000000003\n",
      "episode: 5/1000, score: -18.099999999999987\n",
      "episode: 6/1000, score: 5.500000000000017\n",
      "episode: 7/1000, score: -133.7999999999999\n",
      "episode: 8/1000, score: -8.9\n",
      "episode: 9/1000, score: -16.99999999999999\n",
      "episode: 10/1000, score: -141.99999999999994\n",
      "episode: 11/1000, score: -19.00000000000001\n",
      "episode: 12/1000, score: 37.00000000000002\n",
      "episode: 13/1000, score: -16.99999999999999\n",
      "------------------------------------------------[WIN] Target achieved\n",
      "episode: 14/1000, score: 42.89999999999998\n",
      "episode: 15/1000, score: -118.99999999999991\n",
      "episode: 16/1000, score: -65.99999999999997\n",
      "episode: 17/1000, score: -32.89999999999999\n",
      "episode: 18/1000, score: -22.000000000000025\n",
      "episode: 19/1000, score: -76.99999999999996\n",
      "episode: 20/1000, score: -86.39999999999995\n",
      "episode: 21/1000, score: -30.400000000000016\n",
      "episode: 22/1000, score: -69.99999999999996\n",
      "episode: 23/1000, score: -35.3\n",
      "episode: 24/1000, score: -16.99999999999999\n",
      "episode: 25/1000, score: -82.99999999999991\n",
      "episode: 26/1000, score: -11.000000000000002\n",
      "episode: 27/1000, score: -4.4\n",
      "episode: 28/1000, score: -109.99999999999993\n",
      "episode: 29/1000, score: -35.80000000000003\n",
      "episode: 30/1000, score: 3.0000000000000053\n",
      "episode: 31/1000, score: -10.999999999999996\n",
      "episode: 32/1000, score: -7.6000000000000085\n",
      "episode: 33/1000, score: -78.99999999999997\n",
      "episode: 34/1000, score: -16.99999999999999\n",
      "episode: 35/1000, score: -115.99999999999991\n",
      "episode: 36/1000, score: -131.9999999999999\n",
      "episode: 37/1000, score: -16.99999999999999\n",
      "episode: 38/1000, score: -14.000000000000023\n",
      "episode: 39/1000, score: -107.99999999999993\n",
      "episode: 40/1000, score: -9.600000000000001\n",
      "episode: 41/1000, score: -135.99999999999991\n",
      "episode: 42/1000, score: -15.0\n",
      "episode: 43/1000, score: -46.9\n",
      "episode: 44/1000, score: -19.000000000000007\n",
      "episode: 45/1000, score: -112.99999999999991\n",
      "episode: 46/1000, score: -16.299999999999994\n",
      "episode: 47/1000, score: -73.99999999999997\n",
      "episode: 48/1000, score: -11.600000000000005\n",
      "episode: 49/1000, score: -19.00000000000001\n",
      "episode: 50/1000, score: -23.000000000000018\n",
      "episode: 51/1000, score: -110.89999999999992\n",
      "episode: 52/1000, score: -127.9999999999999\n",
      "episode: 53/1000, score: 8.999999999999972\n",
      "episode: 54/1000, score: -140.99999999999994\n",
      "episode: 55/1000, score: -16.99999999999999\n",
      "episode: 56/1000, score: -97.99999999999993\n",
      "episode: 57/1000, score: -16.99999999999999\n",
      "episode: 58/1000, score: -140.99999999999994\n",
      "episode: 59/1000, score: -138.99999999999991\n",
      "episode: 60/1000, score: -19.000000000000007\n",
      "episode: 61/1000, score: -96.99999999999994\n",
      "episode: 62/1000, score: -53.99999999999999\n",
      "episode: 63/1000, score: -120.99999999999991\n",
      "episode: 64/1000, score: -43.299999999999976\n",
      "episode: 65/1000, score: -16.99999999999999\n",
      "episode: 66/1000, score: -21.000000000000018\n",
      "episode: 67/1000, score: -65.99999999999999\n",
      "episode: 68/1000, score: -39.000000000000014\n",
      "episode: 69/1000, score: -20.80000000000001\n",
      "episode: 70/1000, score: -117.99999999999991\n",
      "episode: 71/1000, score: 1.900000000000011\n",
      "episode: 72/1000, score: -2.599999999999996\n",
      "episode: 73/1000, score: -63.999999999999986\n",
      "episode: 74/1000, score: -104.99999999999993\n",
      "episode: 75/1000, score: -16.99999999999999\n",
      "episode: 76/1000, score: -11.600000000000005\n",
      "episode: 77/1000, score: -132.9999999999999\n",
      "episode: 78/1000, score: -111.99999999999991\n",
      "episode: 79/1000, score: -16.499999999999993\n",
      "episode: 80/1000, score: -3.9000000000000057\n",
      "episode: 81/1000, score: -44.00000000000001\n",
      "episode: 82/1000, score: -126.9999999999999\n",
      "episode: 83/1000, score: -5.900000000000009\n",
      "episode: 84/1000, score: -15.999999999999996\n",
      "episode: 85/1000, score: -33.299999999999976\n",
      "episode: 86/1000, score: -90.79999999999993\n",
      "episode: 87/1000, score: -77.99999999999997\n",
      "episode: 88/1000, score: -16.99999999999999\n",
      "episode: 89/1000, score: -8.900000000000004\n",
      "episode: 90/1000, score: -16.99999999999999\n",
      "episode: 91/1000, score: -137.99999999999991\n",
      "episode: 92/1000, score: -5.3000000000000025\n",
      "episode: 93/1000, score: -16.99999999999999\n",
      "episode: 94/1000, score: -133.9999999999999\n",
      "episode: 95/1000, score: -123.9999999999999\n",
      "episode: 96/1000, score: 25.000000000000007\n",
      "episode: 97/1000, score: -92.99999999999994\n",
      "episode: 98/1000, score: -123.9999999999999\n",
      "episode: 99/1000, score: -77.99999999999996\n",
      "episode: 100/1000, score: -141.99999999999994\n",
      "episode: 101/1000, score: -50.999999999999915\n",
      "episode: 102/1000, score: -25.00000000000002\n",
      "episode: 103/1000, score: -125.0\n",
      "episode: 104/1000, score: -16.99999999999999\n",
      "episode: 105/1000, score: -16.99999999999999\n",
      "episode: 106/1000, score: -27.00000000000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 107/1000, score: -16.99999999999999\n",
      "episode: 108/1000, score: 7.600000000000004\n",
      "episode: 109/1000, score: -109.99999999999991\n",
      "episode: 110/1000, score: -27.000000000000025\n",
      "episode: 111/1000, score: -23.599999999999966\n",
      "episode: 112/1000, score: -10.700000000000005\n",
      "episode: 113/1000, score: -20.699999999999978\n",
      "episode: 114/1000, score: -17.000000000000014\n",
      "episode: 115/1000, score: -27.000000000000032\n",
      "episode: 116/1000, score: -64.99999999999997\n",
      "episode: 117/1000, score: -16.99999999999999\n",
      "episode: 118/1000, score: -38.00000000000002\n",
      "episode: 119/1000, score: -23.00000000000001\n",
      "episode: 120/1000, score: -19.19999999999998\n",
      "episode: 121/1000, score: -128.99999999999991\n",
      "episode: 122/1000, score: -133.9999999999999\n",
      "episode: 123/1000, score: -16.099999999999994\n",
      "episode: 124/1000, score: -135.99999999999991\n",
      "episode: 125/1000, score: -24.899999999999967\n",
      "episode: 126/1000, score: -119.99999999999991\n",
      "episode: 127/1000, score: -17.000000000000018\n",
      "episode: 128/1000, score: -16.99999999999999\n",
      "episode: 129/1000, score: -1.0000000000000204\n",
      "episode: 130/1000, score: -74.99999999999997\n",
      "episode: 131/1000, score: -130.99999999999991\n",
      "episode: 132/1000, score: -23.100000000000012\n",
      "episode: 133/1000, score: -16.99999999999999\n",
      "episode: 134/1000, score: -74.99999999999994\n",
      "episode: 135/1000, score: -77.99999999999996\n",
      "episode: 136/1000, score: -22.59999999999999\n",
      "episode: 137/1000, score: -115.99999999999991\n",
      "episode: 138/1000, score: -16.99999999999999\n",
      "episode: 139/1000, score: -138.99999999999991\n",
      "episode: 140/1000, score: -119.99999999999991\n",
      "episode: 141/1000, score: -27.6\n",
      "episode: 142/1000, score: -114.99999999999991\n",
      "episode: 143/1000, score: -10.800000000000022\n",
      "episode: 144/1000, score: -12.300000000000004\n",
      "episode: 145/1000, score: -6.8999999999999995\n",
      "episode: 146/1000, score: -133.9999999999999\n",
      "episode: 147/1000, score: -126.9999999999999\n",
      "episode: 148/1000, score: -87.99999999999994\n",
      "episode: 149/1000, score: -16.99999999999999\n",
      "episode: 150/1000, score: -137.99999999999991\n",
      "episode: 151/1000, score: -141.99999999999994\n",
      "episode: 152/1000, score: -131.9999999999999\n",
      "episode: 153/1000, score: -17.19999999999999\n",
      "episode: 154/1000, score: 6.99999999999999\n",
      "episode: 155/1000, score: -27.000000000000032\n",
      "episode: 156/1000, score: -28.000000000000036\n",
      "episode: 157/1000, score: -9.0\n",
      "episode: 158/1000, score: -16.99999999999999\n",
      "episode: 159/1000, score: -19.19999999999998\n",
      "episode: 160/1000, score: -23.50000000000002\n",
      "episode: 161/1000, score: -15.199999999999996\n",
      "episode: 162/1000, score: -22.600000000000023\n",
      "episode: 163/1000, score: -17.1\n",
      "episode: 164/1000, score: -16.99999999999999\n",
      "episode: 165/1000, score: -17.09999999999999\n",
      "episode: 166/1000, score: -81.99999999999996\n",
      "episode: 167/1000, score: 21.000000000000004\n",
      "episode: 168/1000, score: -105.99999999999991\n",
      "episode: 169/1000, score: -51.199999999999996\n",
      "episode: 170/1000, score: -36.89999999999999\n",
      "episode: 171/1000, score: -91.99999999999994\n",
      "episode: 172/1000, score: -16.99999999999999\n",
      "episode: 173/1000, score: -89.99999999999994\n",
      "episode: 174/1000, score: -60.799999999999955\n",
      "episode: 175/1000, score: -14.599999999999998\n",
      "episode: 176/1000, score: -8.3\n",
      "episode: 177/1000, score: -128.99999999999991\n",
      "episode: 178/1000, score: -17.899999999999988\n",
      "episode: 179/1000, score: -125.2\n",
      "episode: 180/1000, score: -2.8\n",
      "episode: 181/1000, score: -12.800000000000004\n",
      "episode: 182/1000, score: -16.99999999999999\n",
      "episode: 183/1000, score: -16.99999999999999\n",
      "episode: 184/1000, score: -42.999999999999964\n",
      "episode: 185/1000, score: -5.3000000000000025\n",
      "episode: 186/1000, score: -16.099999999999994\n",
      "episode: 187/1000, score: -32.99999999999998\n",
      "episode: 188/1000, score: -125.9999999999999\n",
      "episode: 189/1000, score: -91.99999999999994\n",
      "episode: 190/1000, score: -59.99999999999998\n",
      "episode: 191/1000, score: -60.99999999999998\n",
      "episode: 192/1000, score: -138.99999999999991\n",
      "episode: 193/1000, score: -16.99999999999999\n",
      "episode: 194/1000, score: -16.99999999999999\n",
      "episode: 195/1000, score: 6.4999999999999725\n",
      "episode: 196/1000, score: -12.999999999999996\n",
      "episode: 197/1000, score: -13.400000000000006\n",
      "episode: 198/1000, score: -49.99999999999999\n",
      "episode: 199/1000, score: -108.99999999999991\n",
      "episode: 200/1000, score: -133.9999999999999\n",
      "episode: 201/1000, score: -109.99999999999991\n",
      "episode: 202/1000, score: -16.39999999999999\n",
      "episode: 203/1000, score: -99.99999999999991\n",
      "episode: 204/1000, score: -131.9999999999999\n",
      "episode: 205/1000, score: -114.99999999999991\n",
      "episode: 206/1000, score: -27.09999999999997\n",
      "episode: 207/1000, score: -140.99999999999994\n",
      "episode: 208/1000, score: -68.99999999999999\n",
      "episode: 209/1000, score: -15.900000000000002\n",
      "episode: 210/1000, score: -125.0\n",
      "episode: 211/1000, score: -16.299999999999994\n",
      "episode: 212/1000, score: -59.40000000000005\n",
      "episode: 213/1000, score: -23.000000000000004\n",
      "episode: 214/1000, score: -133.9999999999999\n",
      "episode: 215/1000, score: -140.99999999999994\n",
      "episode: 216/1000, score: -15.199999999999996\n",
      "episode: 217/1000, score: -131.8999999999999\n",
      "episode: 218/1000, score: -74.99999999999996\n",
      "episode: 219/1000, score: -122.2\n",
      "episode: 220/1000, score: -37.00000000000002\n",
      "episode: 221/1000, score: -130.99999999999991\n",
      "------------------------------------------------[WIN] Target achieved\n",
      "episode: 222/1000, score: 32.89999999999999\n",
      "episode: 223/1000, score: -16.99999999999999\n",
      "episode: 224/1000, score: -23.00000000000002\n",
      "episode: 225/1000, score: -16.099999999999994\n",
      "episode: 226/1000, score: -111.99999999999991\n",
      "episode: 227/1000, score: 22.999999999999968\n",
      "episode: 228/1000, score: -16.99999999999999\n",
      "episode: 229/1000, score: -45.89999999999999\n",
      "episode: 230/1000, score: -133.9999999999999\n",
      "episode: 231/1000, score: -16.99999999999999\n",
      "episode: 232/1000, score: -12.100000000000005\n",
      "episode: 233/1000, score: -14.999999999999996\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6c468536b34c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#agent.run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/GridWorldDQN__CNN.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-91b5d9b652b1>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, Model_name)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/folder/public_transport/GridWorld/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/folder/public_transport/GridWorld/environment.py\u001b[0m in \u001b[0;36mrun_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m                   \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_missed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_achieved\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                   \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_second\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_performed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                   (self.total_reward), debug_mode=(self.debug_mode))\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_pos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/folder/public_transport/GridWorld/frame_runner.py\u001b[0m in \u001b[0;36mrunner\u001b[0;34m(grid, agent_pos, target_pos, timestep, transport_timetable, n_missed, n_achieved, n_iteration, frame_second, reward_action, possible_reward, action_performed, total_reward, debug_mode)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBLACK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mplot_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "debug_mode = False\n",
    "show_graph_every = 1\n",
    "means = False\n",
    "\n",
    "env = GridWorld(show_graph_every, debug_mode, means)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'GridWorld'\n",
    "    agent = DQNAgent(env_name, env)\n",
    "    #agent.run()\n",
    "    agent.test('models/GridWorldDQN__CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
