{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__ is 2.1.0\n",
      "tf.keras.__version__ is: 2.2.4-tf\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "\n",
    "print(\"tf.__version__ is\", tf.__version__)\n",
    "print(\"tf.keras.__version__ is:\", tf.keras.__version__)\n",
    "\n",
    "def _get_available_gpus():\n",
    "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
    "\n",
    "    # Returns\n",
    "        A list of available GPU devices.\n",
    "    \"\"\"\n",
    "    #global _LOCAL_DEVICES\n",
    "    if tfback._LOCAL_DEVICES is None:\n",
    "        devices = tf.config.list_logical_devices()\n",
    "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
    "\n",
    "tfback._get_available_gpus = _get_available_gpus\n",
    "\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lorenzo/anaconda3/envs/lorenzo_env/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pylab\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "from environment import GridWorld\n",
    "\n",
    "\n",
    "def OurModel(input_shape, action_space, dueling):\n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    #X = Conv2D(64, 5, strides=(3, 3),padding=\"valid\", input_shape=input_shape, activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(32, 8, strides=(4, 4),padding=\"valid\", input_shape=input_shape, activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(64, 4, strides=(2, 2),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Conv2D(64, 3, strides=(1, 1),padding=\"valid\", activation=\"relu\", data_format=\"channels_first\")(X)\n",
    "    X = Flatten()(X)\n",
    "    \n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    X = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    if dueling:\n",
    "        state_value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "        state_value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(action_space,))(state_value)\n",
    "\n",
    "        action_advantage = Dense(action_space, kernel_initializer='he_uniform')(X)\n",
    "        action_advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(action_space,))(action_advantage)\n",
    "\n",
    "        X = Add()([state_value, action_advantage])\n",
    "    else:\n",
    "        # Output Layer with # of actions: 2 nodes (left, right)\n",
    "        X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    #model.compile(loss=\"mean_squared_error\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "    #model.compile(optimizer=Adam(lr=0.00025), loss='mean_squared_error')\n",
    "    model.compile(optimizer=Adam(lr=0.00005), loss='mean_squared_error')\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env_name, env):\n",
    "        self.env_name = env_name       \n",
    "        self.env = env\n",
    "        self.action_size = 8\n",
    "        self.EPISODES = 1000\n",
    "        \n",
    "        # Instantiate memory\n",
    "        memory_size = 25000\n",
    "        self.MEMORY = Memory(memory_size)\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "        self.gamma = 0.99    # discount rate\n",
    "        \n",
    "        # EXPLORATION HYPERPARAMETERS for epsilon and epsilon greedy strategy\n",
    "        self.epsilon = 1.0  # exploration probability at start\n",
    "        self.epsilon_min = 0.02  # minimum exploration probability \n",
    "        self.epsilon_decay = 0.00002  # exponential decay rate for exploration prob\n",
    "        \n",
    "        self.batch_size = 32\n",
    "\n",
    "        # defining model parameters\n",
    "        self.ddqn = False # use doudle deep q network\n",
    "        self.dueling = False # use dealing netowrk\n",
    "        self.epsilon_greedy = False # use epsilon greedy strategy\n",
    "        self.USE_PER = False # use priority experienced replay\n",
    "        \n",
    "\n",
    "        self.Save_Path = 'models'\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name+\"_CNN.h5\")\n",
    "\n",
    "        self.ROWS = 50\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 1\n",
    "        self.update_model_steps = 1000\n",
    "        \n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "        \n",
    "        # create main model and target model\n",
    "        self.model = OurModel(input_shape=self.state_size, action_space = self.action_size, dueling = self.dueling)\n",
    "        self.target_model = OurModel(input_shape=self.state_size, action_space = self.action_size, dueling = self.dueling)  \n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self, game_steps):\n",
    "        if game_steps % self.update_model_steps == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            return\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        experience = state, action, reward, next_state, done\n",
    "        if self.USE_PER:\n",
    "            self.MEMORY.store(experience)\n",
    "        else:\n",
    "            self.memory.append((experience))\n",
    "\n",
    "    def act(self, state, decay_step):\n",
    "        # EPSILON GREEDY STRATEGY\n",
    "        if self.epsilon_greedy:\n",
    "        # Here we'll use an improved version of our epsilon greedy strategy for Q-learning\n",
    "            explore_probability = self.epsilon_min + (self.epsilon - self.epsilon_min) * np.exp(-self.epsilon_decay * decay_step)\n",
    "        # OLD EPSILON STRATEGY\n",
    "        else:\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= (1-self.epsilon_decay)\n",
    "            explore_probability = self.epsilon\n",
    "    \n",
    "        if explore_probability > np.random.rand():\n",
    "            # Make a random action (exploration)\n",
    "            return random.randrange(self.action_size), explore_probability\n",
    "        else:\n",
    "            # Get action from Q-network (exploitation)\n",
    "            # Estimate the Qs values state\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            return np.argmax(self.model.predict(state)), explore_probability\n",
    "                \n",
    "    def replay(self):\n",
    "        if self.USE_PER:\n",
    "            # Sample minibatch from the PER memory\n",
    "            tree_idx, minibatch  = self.MEMORY.sample(self.batch_size)\n",
    "        else:\n",
    "            if len(self.memory) > self.batch_size:\n",
    "            # Randomly sample minibatch from the deque memory\n",
    "                minibatch = random.sample(self.memory, self.batch_size)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "        state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        action = np.zeros(self.batch_size, dtype=np.int32)\n",
    "        reward = np.zeros(self.batch_size, dtype=np.float32)\n",
    "        next_state = np.zeros((self.batch_size, *self.state_size), dtype=np.float32)\n",
    "        done = np.zeros(self.batch_size, dtype=np.uint8)\n",
    "\n",
    "        # do this before prediction\n",
    "        # for speedup, this could be done on the tensor level\n",
    "        # but easier to understand using a loop       \n",
    "        for i in range(len(minibatch)):\n",
    "            state[i], action[i], reward[i], next_state[i], done[i] = minibatch[i]\n",
    "\n",
    "        # do batch prediction to save speed\n",
    "        # predict Q-values for starting state using the main network\n",
    "        target = self.model.predict(state)\n",
    "        target_old = np.array(target)\n",
    "        # predict best action in ending state using the main network\n",
    "        target_next = self.model.predict(next_state)\n",
    "        # predict Q-values for ending state using the target network\n",
    "        target_val = self.target_model.predict(next_state)\n",
    "\n",
    "\n",
    "        for i in range(len(minibatch)):\n",
    "            # correction on the Q value for the action used\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                # the key point of Double DQN\n",
    "                # selection of action is from model\n",
    "                # update is from target model\n",
    "                if self.ddqn: # Double - DQN\n",
    "                    # current Q Network selects the action\n",
    "                    # a'_max = argmax_a' Q(s', a')\n",
    "                    a = np.argmax(target_next[i])\n",
    "                    # target Q Network evaluates the action\n",
    "                    # Q_max = Q_target(s', a'_max)\n",
    "                    target[i][action[i]] = reward[i] + self.gamma * target_val[i][a]\n",
    "                else: # Standard - DQN\n",
    "                    # DQN chooses the max Q value among next actions\n",
    "                    # selection and evaluation of action is on the target Q Network\n",
    "                    # Q_max = max_a' Q_target(s', a')\n",
    "                    # when using target model in simple DQN rules, we get better performance\n",
    "                    target[i][action[i]] = reward[i] + self.gamma * np.amax(target_val[i])\n",
    "            \n",
    "        if self.USE_PER:\n",
    "            indices = np.arange(self.batch_size, dtype=np.int32)\n",
    "            absolute_errors = np.abs(target_old[indices, action]-target[indices, action])\n",
    "\n",
    "            # Update priority\n",
    "            self.MEMORY.batch_update(tree_idx, absolute_errors)\n",
    "                \n",
    "        # Train the Neural Network with batches\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name, by_name=False)\n",
    "\n",
    "    def save(self, name):\n",
    "        model = self.model\n",
    "        model.save_weights(name)\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    plt.figure(figsize=(18,9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        pylab.plot(self.episodes, self.average, 'r')\n",
    "        pylab.plot(self.episodes, self.scores, 'b')\n",
    "        pylab.ylabel('Score', fontsize=18)\n",
    "        pylab.xlabel('Games', fontsize=18)\n",
    "        dqn = 'DQN_'\n",
    "        dueling = ''\n",
    "        greedy = ''\n",
    "        PER = ''\n",
    "        if self.ddqn: dqn = '_DDQN'\n",
    "        if self.dueling: dueling = '_Dueling'\n",
    "        if self.epsilon_greedy: greedy = '_Greedy'\n",
    "        if self.USE_PER: PER = '_PER'\n",
    "        try:\n",
    "            if not os.path.exists('training_images'): os.makedirs('training_images')\n",
    "            pylab.savefig('training_images/'+self.env_name+dqn+dueling+greedy+PER+\"_CNN.png\")\n",
    "            \n",
    "        except OSError as e:\n",
    "            pass\n",
    "        # no need to worry about model, when doing a lot of experiments\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.env_name+dqn+dueling+greedy+PER+\"_CNN.h5\")\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(\"cartpole\"+str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def GetImage(self, frame):\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        self.image_memory[0,:,:] = frame\n",
    "\n",
    "        # show image frame   \n",
    "        #self.imshow(self.image_memory,0)\n",
    "        #self.imshow(self.image_memory,1)\n",
    "        #self.imshow(self.image_memory,2)\n",
    "        #self.imshow(self.image_memory,3)\n",
    "        \n",
    "        return np.expand_dims(self.image_memory, axis=0)\n",
    "\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(frame)\n",
    "        return state\n",
    "\n",
    "    def step(self,action):\n",
    "        info = None\n",
    "        state, reward, total_reward, next_state, done = self.env.step(action)\n",
    "        next_state = self.GetImage(next_state)\n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def run(self):\n",
    "        decay_step = 0\n",
    "        max_average = -21.0\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            SAVING = ''\n",
    "            while not done:\n",
    "                decay_step += 1\n",
    "                action, explore_probability = self.act(state, decay_step)\n",
    "                next_state, reward, done, _ = self.step(action)\n",
    "                '''\n",
    "                if reward != 0.0:\n",
    "                    print('States -------------------------------------------------------------------------')\n",
    "                    plt.imshow(state[0][0])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][1])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][2])\n",
    "                    plt.show()\n",
    "                    plt.imshow(state[0][3])\n",
    "                    plt.show()\n",
    "                    print('Action:', action)\n",
    "                    print('Reward:', reward)\n",
    "                    '''\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    # every episode, plot the result\n",
    "                    average = self.PlotModel(score, e)\n",
    "\n",
    "                    # saving best models\n",
    "                    if average >= max_average:\n",
    "                        max_average = average\n",
    "                        self.save(self.Model_name)\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2f}, average: {:.2f} {}\".format(e, self.EPISODES, score, explore_probability, average, SAVING))\n",
    "                    \n",
    "                # update target model\n",
    "                self.update_target_model(decay_step)\n",
    "\n",
    "                # train model\n",
    "                self.replay()\n",
    "\n",
    "        # close environemnt when finish training\n",
    "        #self.env.close()\n",
    "\n",
    "    def test(self, Model_name):\n",
    "        import time\n",
    "        self.load(Model_name)\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                time.sleep(.01)\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                state, reward, total_reward, next_state, done = self.env.step(action)\n",
    "                state = self.GetImage(next_state)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "        #self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1, 50, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 32, 11, 19)        2080      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, 4, 8)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 64, 2, 6)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 613,864\n",
      "Trainable params: 613,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 1, 50, 80)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 32, 11, 19)        2080      \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 64, 4, 8)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 64, 2, 6)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 613,864\n",
      "Trainable params: 613,864\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0/1000, score: -5.0\n",
      "episode: 1/1000, score: -23.00000000000002\n",
      "episode: 2/1000, score: -10.999999999999995\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6c468536b34c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#agent.run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/GridWorldDQN__CNN.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-4abe1edcdc4d>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, Model_name)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/folder/public_transport/GridWorld/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/folder/public_transport/GridWorld/environment.py\u001b[0m in \u001b[0;36mrun_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m             fr.runner((self.grid), (self.agent_pos), (self.target_pos), (self.timestep), (self.transport_timetable),\n\u001b[1;32m    201\u001b[0m                   \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_missed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_achieved\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                   \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_second\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_performed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m                   (self.total_reward), debug_mode=(self.debug_mode))\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_pos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/folder/public_transport/GridWorld/frame_runner.py\u001b[0m in \u001b[0;36mrunner\u001b[0;34m(grid, agent_pos, target_pos, timestep, transport_timetable, n_missed, n_achieved, n_iteration, frame_second, reward_action, possible_reward, action_performed, total_reward, debug_mode)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBLACK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mplot_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time: '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "debug_mode = False\n",
    "show_graph_every = 1\n",
    "means = False\n",
    "\n",
    "env = GridWorld(show_graph_every, debug_mode, means)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'GridWorld'\n",
    "    agent = DQNAgent(env_name, env)\n",
    "    #agent.run()\n",
    "    agent.test('models/GridWorldDQN__CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
